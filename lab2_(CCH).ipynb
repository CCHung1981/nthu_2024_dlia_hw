{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c",
      "metadata": {
        "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "###### Lab 2\n",
        "\n",
        "# National Tsing Hua University\n",
        "\n",
        "#### Spring 2024\n",
        "\n",
        "#### 11220IEEM 513600\n",
        "\n",
        "#### Deep Learning and Industrial Applications\n",
        "    \n",
        "## Lab 2: Predicting Heart Disease with Deep Learning\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963",
      "metadata": {
        "tags": [],
        "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "In the realm of healthcare, early detection and accurate prediction of diseases play a crucial role in patient care and management. Heart disease remains one of the leading causes of mortality worldwide, making the development of effective diagnostic tools essential. This lab leverages deep learning to predict the presence of heart disease in patients using a subset of 14 key attributes from the Cleveland Heart Disease Database. The objective is to explore and apply deep learning techniques to distinguish between the presence and absence of heart disease based on clinical parameters.\n",
        "\n",
        "Throughout this lab, you'll engage with the following key activities:\n",
        "- Use [Pandas](https://pandas.pydata.org) to process the CSV files.\n",
        "- Use [PyTorch](https://pytorch.org) to build an Artificial Neural Network (ANN) to fit the dataset.\n",
        "- Evaluate the performance of the trained model to understand its accuracy.\n",
        "\n",
        "### Attribute Information\n",
        "\n",
        "1. age: Age of the patient in years\n",
        "2. sex: (Male/Female)\n",
        "3. cp: Chest pain type (4 types: low, medium, high, and severe)\n",
        "4. trestbps: Resting blood pressure\n",
        "5. chol: Serum cholesterol in mg/dl\n",
        "6. fbs: Fasting blood sugar > 120 mg/dl\n",
        "7. restecg: Resting electrocardiographic results (values 0,1,2)\n",
        "8. thalach: Maximum heart rate achieved\n",
        "9. exang: Exercise induced angina\n",
        "10. oldpeak: Oldpeak = ST depression induced by exercise relative to rest\n",
        "11. slope: The slope of the peak exercise ST segment\n",
        "12. ca: Number of major vessels (0-3) colored by fluoroscopy\n",
        "13. thal: 3 = normal; 6 = fixed defect; 7 = reversible defect\n",
        "14. target: target have disease or not (1=yes, 0=no)\n",
        "\n",
        "### References\n",
        "- [UCI Heart Disease Data](https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data) for the dataset we use in this lab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad594fc8-4989-40f3-b124-4550fe7df386",
      "metadata": {
        "id": "ad594fc8-4989-40f3-b124-4550fe7df386"
      },
      "source": [
        "## A. Checking and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "42a3eafd-cbcd-4c56-82cb-83a0bfa2399e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "42a3eafd-cbcd-4c56-82cb-83a0bfa2399e",
        "outputId": "5dd10026-a4c6-478b-8768-2eb71558cb6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     age     sex      cp  trestbps   chol  fbs  restecg  thalach  exang  \\\n",
              "0     41    Male  medium     105.0  198.0    0      1.0    168.0      0   \n",
              "1     65  Female     low     120.0  177.0    0      1.0    140.0      0   \n",
              "2     44  Female  medium     130.0  219.0    0      0.0    188.0      0   \n",
              "3     54  Female    high     125.0  273.0    0      0.0    152.0      0   \n",
              "4     51  Female  severe     125.0  213.0    0      0.0    125.0      1   \n",
              "..   ...     ...     ...       ...    ...  ...      ...      ...    ...   \n",
              "268   40  Female     low     110.0  167.0    0      0.0    114.0      1   \n",
              "269   60  Female     low     117.0  230.0    1      1.0    160.0      1   \n",
              "270   64  Female    high     140.0  335.0    0      1.0    158.0      0   \n",
              "271   43  Female     low     120.0  177.0    0      0.0    120.0      1   \n",
              "272   57  Female     low     150.0  276.0    0      0.0    112.0      1   \n",
              "\n",
              "     oldpeak  slope  ca  thal  target  \n",
              "0        0.0    2.0   1   2.0     1.0  \n",
              "1        0.4    2.0   0   3.0     1.0  \n",
              "2        0.0    2.0   0   2.0     1.0  \n",
              "3        0.5    0.0   1   2.0     1.0  \n",
              "4        1.4    2.0   1   2.0     1.0  \n",
              "..       ...    ...  ..   ...     ...  \n",
              "268      2.0    1.0   0   3.0     0.0  \n",
              "269      1.4    2.0   2   3.0     0.0  \n",
              "270      0.0    2.0   0   2.0     0.0  \n",
              "271      2.5    1.0   0   3.0     0.0  \n",
              "272      0.6    1.0   1   1.0     0.0  \n",
              "\n",
              "[273 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb326457-c1d0-4c87-83d6-bd922644c9ee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>41</td>\n",
              "      <td>Male</td>\n",
              "      <td>medium</td>\n",
              "      <td>105.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44</td>\n",
              "      <td>Female</td>\n",
              "      <td>medium</td>\n",
              "      <td>130.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>Female</td>\n",
              "      <td>high</td>\n",
              "      <td>125.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>Female</td>\n",
              "      <td>severe</td>\n",
              "      <td>125.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>40</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>110.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>60</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>117.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>64</td>\n",
              "      <td>Female</td>\n",
              "      <td>high</td>\n",
              "      <td>140.0</td>\n",
              "      <td>335.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>43</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>57</td>\n",
              "      <td>Female</td>\n",
              "      <td>low</td>\n",
              "      <td>150.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>273 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb326457-c1d0-4c87-83d6-bd922644c9ee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb326457-c1d0-4c87-83d6-bd922644c9ee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb326457-c1d0-4c87-83d6-bd922644c9ee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d3405aab-d4df-472e-8c81-9fcdf4e20c6f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d3405aab-d4df-472e-8c81-9fcdf4e20c6f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d3405aab-d4df-472e-8c81-9fcdf4e20c6f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 273,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          60,\n          62,\n          53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Female\",\n          \"Male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"low\",\n          \"severe\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.85239476086147,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          108.0,\n          114.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51.44738664891149,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          262.0,\n          266.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5291071887540659,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.20476792843544,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          106.0,\n          168.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1842964903760378,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.7,\n          4.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.617520158897505,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6220982126950156,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49862791987061383,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('heart_dataset_train_all.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "34241797-60f0-4818-a44b-f5379948d621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34241797-60f0-4818-a44b-f5379948d621",
        "outputId": "92e10c64-2368-48c2-cc83-431b914da80e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
              "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "026585db-a6d8-4062-85de-e3a7eaebed72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "026585db-a6d8-4062-85de-e3a7eaebed72",
        "outputId": "82349f1d-8cef-4ae4-d57d-4a17ce6c3972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 273 entries, 0 to 272\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       273 non-null    int64  \n",
            " 1   sex       272 non-null    object \n",
            " 2   cp        272 non-null    object \n",
            " 3   trestbps  272 non-null    float64\n",
            " 4   chol      271 non-null    float64\n",
            " 5   fbs       273 non-null    int64  \n",
            " 6   restecg   272 non-null    float64\n",
            " 7   thalach   272 non-null    float64\n",
            " 8   exang     273 non-null    int64  \n",
            " 9   oldpeak   273 non-null    float64\n",
            " 10  slope     271 non-null    float64\n",
            " 11  ca        273 non-null    int64  \n",
            " 12  thal      272 non-null    float64\n",
            " 13  target    272 non-null    float64\n",
            "dtypes: float64(8), int64(4), object(2)\n",
            "memory usage: 30.0+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "69031e6d-0fb5-49d9-b723-a0d1fee08c3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69031e6d-0fb5-49d9-b723-a0d1fee08c3c",
        "outputId": "884813dd-20bd-465c-aeb7-36dedcb05bdf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         1\n",
              "cp          1\n",
              "trestbps    1\n",
              "chol        2\n",
              "fbs         0\n",
              "restecg     1\n",
              "thalach     1\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       2\n",
              "ca          0\n",
              "thal        1\n",
              "target      1\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# checking for null values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "cb3090f8-2cfa-4f56-8aa5-cf954bb19932",
      "metadata": {
        "id": "cb3090f8-2cfa-4f56-8aa5-cf954bb19932"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "38aadbee-d68f-4ae0-b842-b40800b0cac9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38aadbee-d68f-4ae0-b842-b40800b0cac9",
        "outputId": "a95f41a9-ede8-4082-afd5-d5d2404d3f15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(270, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "26a69fd5-3534-4d8e-b59a-6778bf47a479",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "26a69fd5-3534-4d8e-b59a-6778bf47a479",
        "outputId": "09d6fa51-0d3d-45ff-fbba-224fda9ab379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-39abb612ef0e>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'sex'] = df['sex'].map(sex_description)\n",
            "<ipython-input-48-39abb612ef0e>:6: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df.loc[:, 'sex'] = df['sex'].map(sex_description)\n",
            "<ipython-input-48-39abb612ef0e>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, 'cp'] = df['cp'].map(pain_description)\n",
            "<ipython-input-48-39abb612ef0e>:15: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df.loc[:, 'cp'] = df['cp'].map(pain_description)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     41    0   1     105.0  198.0    0      1.0    168.0      0      0.0   \n",
              "1     65    1   0     120.0  177.0    0      1.0    140.0      0      0.4   \n",
              "2     44    1   1     130.0  219.0    0      0.0    188.0      0      0.0   \n",
              "3     54    1   2     125.0  273.0    0      0.0    152.0      0      0.5   \n",
              "4     51    1   3     125.0  213.0    0      0.0    125.0      1      1.4   \n",
              "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...   \n",
              "268   40    1   0     110.0  167.0    0      0.0    114.0      1      2.0   \n",
              "269   60    1   0     117.0  230.0    1      1.0    160.0      1      1.4   \n",
              "270   64    1   2     140.0  335.0    0      1.0    158.0      0      0.0   \n",
              "271   43    1   0     120.0  177.0    0      0.0    120.0      1      2.5   \n",
              "272   57    1   0     150.0  276.0    0      0.0    112.0      1      0.6   \n",
              "\n",
              "     slope  ca  thal  target  \n",
              "0      2.0   1   2.0     1.0  \n",
              "1      2.0   0   3.0     1.0  \n",
              "2      2.0   0   2.0     1.0  \n",
              "3      0.0   1   2.0     1.0  \n",
              "4      2.0   1   2.0     1.0  \n",
              "..     ...  ..   ...     ...  \n",
              "268    1.0   0   3.0     0.0  \n",
              "269    2.0   2   3.0     0.0  \n",
              "270    2.0   0   2.0     0.0  \n",
              "271    1.0   0   3.0     0.0  \n",
              "272    1.0   1   1.0     0.0  \n",
              "\n",
              "[270 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03e92d72-f838-4866-a87d-8add1a8466d8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>105.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>130.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>125.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>125.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>335.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>270 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03e92d72-f838-4866-a87d-8add1a8466d8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-03e92d72-f838-4866-a87d-8add1a8466d8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-03e92d72-f838-4866-a87d-8add1a8466d8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-729a071e-7efc-4a42-a680-c8e7a34b11ef\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-729a071e-7efc-4a42-a680-c8e7a34b11ef')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-729a071e-7efc-4a42-a680-c8e7a34b11ef button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 270,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 29,\n        \"max\": 77,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          60,\n          62,\n          39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.904675150980836,\n        \"min\": 94.0,\n        \"max\": 200.0,\n        \"num_unique_values\": 49,\n        \"samples\": [\n          108.0,\n          114.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51.52941143001837,\n        \"min\": 126.0,\n        \"max\": 564.0,\n        \"num_unique_values\": 145,\n        \"samples\": [\n          262.0,\n          266.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5293141619418642,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.217253005462286,\n        \"min\": 71.0,\n        \"max\": 202.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          106.0,\n          168.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.188378543758624,\n        \"min\": 0.0,\n        \"max\": 6.2,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.7,\n          4.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6181877820120649,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6238744511959265,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49894560448305547,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# Mapping 'sex' descriptions to numbers\n",
        "sex_description = {\n",
        "    'Male': 0,\n",
        "    'Female': 1,\n",
        "}\n",
        "df.loc[:, 'sex'] = df['sex'].map(sex_description)\n",
        "\n",
        "# Mapping 'cp' (chest pain) descriptions to numbers\n",
        "pain_description = {\n",
        "    'low': 0,\n",
        "    'medium': 1,\n",
        "    'high': 2,\n",
        "    'severe': 3\n",
        "}\n",
        "df.loc[:, 'cp'] = df['cp'].map(pain_description)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "051108c6-7011-4187-9e36-bd2944a019ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "051108c6-7011-4187-9e36-bd2944a019ca",
        "outputId": "d8d16ceb-8ec7-4ecb-e719-99adf62badcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              age         sex          cp    trestbps        chol         fbs  \\\n",
              "count  270.000000  270.000000  270.000000  270.000000  270.000000  270.000000   \n",
              "mean    54.385185    0.685185    0.962963  131.525926  245.607407    0.151852   \n",
              "std      9.149713    0.465305    1.023206   17.904675   51.529411    0.359544   \n",
              "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
              "25%     47.250000    0.000000    0.000000  120.000000  210.250000    0.000000   \n",
              "50%     56.000000    1.000000    1.000000  130.000000  240.500000    0.000000   \n",
              "75%     61.000000    1.000000    2.000000  140.000000  274.000000    0.000000   \n",
              "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
              "\n",
              "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
              "count  270.000000  270.000000  270.000000  270.000000  270.000000  270.000000   \n",
              "mean     0.522222  149.807407    0.333333    1.024074    1.400000    0.744444   \n",
              "std      0.529314   23.217253    0.472280    1.188379    0.618188    1.037166   \n",
              "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "25%      0.000000  134.500000    0.000000    0.000000    1.000000    0.000000   \n",
              "50%      1.000000  152.500000    0.000000    0.600000    1.000000    0.000000   \n",
              "75%      1.000000  167.750000    1.000000    1.600000    2.000000    1.000000   \n",
              "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
              "\n",
              "             thal      target  \n",
              "count  270.000000  270.000000  \n",
              "mean     2.300000    0.544444  \n",
              "std      0.623874    0.498946  \n",
              "min      0.000000    0.000000  \n",
              "25%      2.000000    0.000000  \n",
              "50%      2.000000    1.000000  \n",
              "75%      3.000000    1.000000  \n",
              "max      3.000000    1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52bcf086-a40a-4204-8ec3-ae5fdc979ec9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>54.385185</td>\n",
              "      <td>0.685185</td>\n",
              "      <td>0.962963</td>\n",
              "      <td>131.525926</td>\n",
              "      <td>245.607407</td>\n",
              "      <td>0.151852</td>\n",
              "      <td>0.522222</td>\n",
              "      <td>149.807407</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.024074</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>0.744444</td>\n",
              "      <td>2.300000</td>\n",
              "      <td>0.544444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.149713</td>\n",
              "      <td>0.465305</td>\n",
              "      <td>1.023206</td>\n",
              "      <td>17.904675</td>\n",
              "      <td>51.529411</td>\n",
              "      <td>0.359544</td>\n",
              "      <td>0.529314</td>\n",
              "      <td>23.217253</td>\n",
              "      <td>0.472280</td>\n",
              "      <td>1.188379</td>\n",
              "      <td>0.618188</td>\n",
              "      <td>1.037166</td>\n",
              "      <td>0.623874</td>\n",
              "      <td>0.498946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>47.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>210.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>134.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>56.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>240.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>152.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>274.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>167.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>77.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>202.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52bcf086-a40a-4204-8ec3-ae5fdc979ec9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-52bcf086-a40a-4204-8ec3-ae5fdc979ec9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-52bcf086-a40a-4204-8ec3-ae5fdc979ec9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c5af2017-9875-4b6b-90d1-aa6ca87c7d4c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c5af2017-9875-4b6b-90d1-aa6ca87c7d4c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c5af2017-9875-4b6b-90d1-aa6ca87c7d4c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81.26807726653664,\n        \"min\": 9.149713209948988,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          54.385185185185186,\n          56.0,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.25070274123759,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6851851851851852,\n          1.0,\n          0.4653045560611131\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.06120997146988,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          270.0,\n          0.9629629629629629,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73.73772068120547,\n        \"min\": 17.904675150980836,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          131.52592592592592,\n          130.0,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 149.27786697513415,\n        \"min\": 51.52941143001837,\n        \"max\": 564.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          245.6074074074074,\n          240.5,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.38369701469603,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.15185185185185185,\n          1.0,\n          0.3595436702724567\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.2064556868165,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          0.5222222222222223,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75.47345746130111,\n        \"min\": 23.217253005462286,\n        \"max\": 270.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          149.8074074074074,\n          152.5,\n          270.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.31861707775641,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3333333333333333,\n          1.0,\n          0.4722799245548624\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94.94427117355892,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          270.0,\n          1.0240740740740741,\n          1.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.05680862661214,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          1.4,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.1259484289674,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          0.7444444444444445,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94.81255144269919,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          270.0,\n          2.3,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.25610060161932,\n        \"min\": 0.0,\n        \"max\": 270.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.5444444444444444,\n          1.0,\n          0.49894560448305547\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "8b999df5-09a1-4ce2-b068-f1afba448ff8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "8b999df5-09a1-4ce2-b068-f1afba448ff8",
        "outputId": "14bfecf2-8124-4350-d36a-4e2b0ea718f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               age       sex        cp  trestbps      chol       fbs  \\\n",
              "age       1.000000 -0.062222 -0.103697  0.261782  0.210520  0.109847   \n",
              "sex      -0.062222  1.000000 -0.040197 -0.055463 -0.166885  0.042384   \n",
              "cp       -0.103697 -0.040197  1.000000  0.035563 -0.063592  0.065869   \n",
              "trestbps  0.261782 -0.055463  0.035563  1.000000  0.128444  0.170606   \n",
              "chol      0.210520 -0.166885 -0.063592  0.128444  1.000000  0.003430   \n",
              "fbs       0.109847  0.042384  0.065869  0.170606  0.003430  1.000000   \n",
              "restecg  -0.124588 -0.069599  0.008389 -0.145195 -0.162687 -0.086165   \n",
              "thalach  -0.412624 -0.058626  0.300307 -0.056631 -0.023753 -0.014297   \n",
              "exang     0.111263  0.124054 -0.428233  0.067116  0.063902  0.029190   \n",
              "oldpeak   0.200243  0.089726 -0.183616  0.184896  0.084355  0.007943   \n",
              "slope    -0.165360 -0.038771  0.135174 -0.126553 -0.031929 -0.056866   \n",
              "ca        0.254462  0.140795 -0.180598  0.093545  0.068647  0.164266   \n",
              "thal      0.077368  0.198493 -0.139765  0.068690  0.121280 -0.004972   \n",
              "target   -0.244798 -0.283776  0.425574 -0.173239 -0.096773 -0.068845   \n",
              "\n",
              "           restecg   thalach     exang   oldpeak     slope        ca  \\\n",
              "age      -0.124588 -0.412624  0.111263  0.200243 -0.165360  0.254462   \n",
              "sex      -0.069599 -0.058626  0.124054  0.089726 -0.038771  0.140795   \n",
              "cp        0.008389  0.300307 -0.428233 -0.183616  0.135174 -0.180598   \n",
              "trestbps -0.145195 -0.056631  0.067116  0.184896 -0.126553  0.093545   \n",
              "chol     -0.162687 -0.023753  0.063902  0.084355 -0.031929  0.068647   \n",
              "fbs      -0.086165 -0.014297  0.029190  0.007943 -0.056866  0.164266   \n",
              "restecg   1.000000  0.025457 -0.089225 -0.047837  0.074982 -0.053946   \n",
              "thalach   0.025457  1.000000 -0.404349 -0.340564  0.370073 -0.205060   \n",
              "exang    -0.089225 -0.404349  1.000000  0.294308 -0.280124  0.106250   \n",
              "oldpeak  -0.047837 -0.340564  0.294308  1.000000 -0.585472  0.223375   \n",
              "slope     0.074982  0.370073 -0.280124 -0.585472  1.000000 -0.083491   \n",
              "ca       -0.053946 -0.205060  0.106250  0.223375 -0.083491  1.000000   \n",
              "thal     -0.003377 -0.078637  0.189253  0.200315 -0.090606  0.136160   \n",
              "target    0.101817  0.432687 -0.457502 -0.443504  0.363983 -0.391031   \n",
              "\n",
              "              thal    target  \n",
              "age       0.077368 -0.244798  \n",
              "sex       0.198493 -0.283776  \n",
              "cp       -0.139765  0.425574  \n",
              "trestbps  0.068690 -0.173239  \n",
              "chol      0.121280 -0.096773  \n",
              "fbs      -0.004972 -0.068845  \n",
              "restecg  -0.003377  0.101817  \n",
              "thalach  -0.078637  0.432687  \n",
              "exang     0.189253 -0.457502  \n",
              "oldpeak   0.200315 -0.443504  \n",
              "slope    -0.090606  0.363983  \n",
              "ca        0.136160 -0.391031  \n",
              "thal      1.000000 -0.311701  \n",
              "target   -0.311701  1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d992508-4963-448b-83e1-434e6efb9f5b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.062222</td>\n",
              "      <td>-0.103697</td>\n",
              "      <td>0.261782</td>\n",
              "      <td>0.210520</td>\n",
              "      <td>0.109847</td>\n",
              "      <td>-0.124588</td>\n",
              "      <td>-0.412624</td>\n",
              "      <td>0.111263</td>\n",
              "      <td>0.200243</td>\n",
              "      <td>-0.165360</td>\n",
              "      <td>0.254462</td>\n",
              "      <td>0.077368</td>\n",
              "      <td>-0.244798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>-0.062222</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.040197</td>\n",
              "      <td>-0.055463</td>\n",
              "      <td>-0.166885</td>\n",
              "      <td>0.042384</td>\n",
              "      <td>-0.069599</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.124054</td>\n",
              "      <td>0.089726</td>\n",
              "      <td>-0.038771</td>\n",
              "      <td>0.140795</td>\n",
              "      <td>0.198493</td>\n",
              "      <td>-0.283776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cp</th>\n",
              "      <td>-0.103697</td>\n",
              "      <td>-0.040197</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.035563</td>\n",
              "      <td>-0.063592</td>\n",
              "      <td>0.065869</td>\n",
              "      <td>0.008389</td>\n",
              "      <td>0.300307</td>\n",
              "      <td>-0.428233</td>\n",
              "      <td>-0.183616</td>\n",
              "      <td>0.135174</td>\n",
              "      <td>-0.180598</td>\n",
              "      <td>-0.139765</td>\n",
              "      <td>0.425574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trestbps</th>\n",
              "      <td>0.261782</td>\n",
              "      <td>-0.055463</td>\n",
              "      <td>0.035563</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.128444</td>\n",
              "      <td>0.170606</td>\n",
              "      <td>-0.145195</td>\n",
              "      <td>-0.056631</td>\n",
              "      <td>0.067116</td>\n",
              "      <td>0.184896</td>\n",
              "      <td>-0.126553</td>\n",
              "      <td>0.093545</td>\n",
              "      <td>0.068690</td>\n",
              "      <td>-0.173239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chol</th>\n",
              "      <td>0.210520</td>\n",
              "      <td>-0.166885</td>\n",
              "      <td>-0.063592</td>\n",
              "      <td>0.128444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>-0.162687</td>\n",
              "      <td>-0.023753</td>\n",
              "      <td>0.063902</td>\n",
              "      <td>0.084355</td>\n",
              "      <td>-0.031929</td>\n",
              "      <td>0.068647</td>\n",
              "      <td>0.121280</td>\n",
              "      <td>-0.096773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fbs</th>\n",
              "      <td>0.109847</td>\n",
              "      <td>0.042384</td>\n",
              "      <td>0.065869</td>\n",
              "      <td>0.170606</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>-0.014297</td>\n",
              "      <td>0.029190</td>\n",
              "      <td>0.007943</td>\n",
              "      <td>-0.056866</td>\n",
              "      <td>0.164266</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>-0.068845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>restecg</th>\n",
              "      <td>-0.124588</td>\n",
              "      <td>-0.069599</td>\n",
              "      <td>0.008389</td>\n",
              "      <td>-0.145195</td>\n",
              "      <td>-0.162687</td>\n",
              "      <td>-0.086165</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.025457</td>\n",
              "      <td>-0.089225</td>\n",
              "      <td>-0.047837</td>\n",
              "      <td>0.074982</td>\n",
              "      <td>-0.053946</td>\n",
              "      <td>-0.003377</td>\n",
              "      <td>0.101817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thalach</th>\n",
              "      <td>-0.412624</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.300307</td>\n",
              "      <td>-0.056631</td>\n",
              "      <td>-0.023753</td>\n",
              "      <td>-0.014297</td>\n",
              "      <td>0.025457</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.404349</td>\n",
              "      <td>-0.340564</td>\n",
              "      <td>0.370073</td>\n",
              "      <td>-0.205060</td>\n",
              "      <td>-0.078637</td>\n",
              "      <td>0.432687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>exang</th>\n",
              "      <td>0.111263</td>\n",
              "      <td>0.124054</td>\n",
              "      <td>-0.428233</td>\n",
              "      <td>0.067116</td>\n",
              "      <td>0.063902</td>\n",
              "      <td>0.029190</td>\n",
              "      <td>-0.089225</td>\n",
              "      <td>-0.404349</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.294308</td>\n",
              "      <td>-0.280124</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.189253</td>\n",
              "      <td>-0.457502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oldpeak</th>\n",
              "      <td>0.200243</td>\n",
              "      <td>0.089726</td>\n",
              "      <td>-0.183616</td>\n",
              "      <td>0.184896</td>\n",
              "      <td>0.084355</td>\n",
              "      <td>0.007943</td>\n",
              "      <td>-0.047837</td>\n",
              "      <td>-0.340564</td>\n",
              "      <td>0.294308</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.585472</td>\n",
              "      <td>0.223375</td>\n",
              "      <td>0.200315</td>\n",
              "      <td>-0.443504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slope</th>\n",
              "      <td>-0.165360</td>\n",
              "      <td>-0.038771</td>\n",
              "      <td>0.135174</td>\n",
              "      <td>-0.126553</td>\n",
              "      <td>-0.031929</td>\n",
              "      <td>-0.056866</td>\n",
              "      <td>0.074982</td>\n",
              "      <td>0.370073</td>\n",
              "      <td>-0.280124</td>\n",
              "      <td>-0.585472</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.083491</td>\n",
              "      <td>-0.090606</td>\n",
              "      <td>0.363983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca</th>\n",
              "      <td>0.254462</td>\n",
              "      <td>0.140795</td>\n",
              "      <td>-0.180598</td>\n",
              "      <td>0.093545</td>\n",
              "      <td>0.068647</td>\n",
              "      <td>0.164266</td>\n",
              "      <td>-0.053946</td>\n",
              "      <td>-0.205060</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.223375</td>\n",
              "      <td>-0.083491</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.136160</td>\n",
              "      <td>-0.391031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thal</th>\n",
              "      <td>0.077368</td>\n",
              "      <td>0.198493</td>\n",
              "      <td>-0.139765</td>\n",
              "      <td>0.068690</td>\n",
              "      <td>0.121280</td>\n",
              "      <td>-0.004972</td>\n",
              "      <td>-0.003377</td>\n",
              "      <td>-0.078637</td>\n",
              "      <td>0.189253</td>\n",
              "      <td>0.200315</td>\n",
              "      <td>-0.090606</td>\n",
              "      <td>0.136160</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.311701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>-0.244798</td>\n",
              "      <td>-0.283776</td>\n",
              "      <td>0.425574</td>\n",
              "      <td>-0.173239</td>\n",
              "      <td>-0.096773</td>\n",
              "      <td>-0.068845</td>\n",
              "      <td>0.101817</td>\n",
              "      <td>0.432687</td>\n",
              "      <td>-0.457502</td>\n",
              "      <td>-0.443504</td>\n",
              "      <td>0.363983</td>\n",
              "      <td>-0.391031</td>\n",
              "      <td>-0.311701</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d992508-4963-448b-83e1-434e6efb9f5b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6d992508-4963-448b-83e1-434e6efb9f5b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6d992508-4963-448b-83e1-434e6efb9f5b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8418d7ee-f406-4670-9e8e-9356812bc04c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8418d7ee-f406-4670-9e8e-9356812bc04c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8418d7ee-f406-4670-9e8e-9356812bc04c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 14,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3336796953563416,\n        \"min\": -0.4126237683266394,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2002432632272073,\n          0.25446220532709146,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2991228194776611,\n        \"min\": -0.28377582305309207,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.08972560084685732,\n          0.14079450235695648,\n          -0.062222038735579396\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.343237644388663,\n        \"min\": -0.42823328385023884,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.18361571896503148,\n          -0.18059763067401924,\n          -0.10369651400029238\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28956654914616836,\n        \"min\": -0.1732391623681546,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.18489606509122675,\n          0.09354457752219755,\n          0.2617824169771793\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2867254997156597,\n        \"min\": -0.16688487098702884,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.08435532313293413,\n          0.06864714277304984,\n          0.21052008975387562\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27150092525557695,\n        \"min\": -0.08616493327966852,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.00794318047234304,\n          0.16426559207767857,\n          0.10984693693665605\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28984501434053883,\n        \"min\": -0.16268743250369436,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.04783727701622021,\n          -0.05394643396214631,\n          -0.12458764457926447\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.38230319141316005,\n        \"min\": -0.4126237683266394,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.34056397282727374,\n          -0.20506017090677786,\n          -0.4126237683266394\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3727203587084935,\n        \"min\": -0.4575020365957928,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2943081762485106,\n          0.10624980942135133,\n          0.11126311741056943\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.38472554244127527,\n        \"min\": -0.5854716356239958,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          1.0,\n          0.22337523920060878,\n          0.2002432632272073\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.36816285406256916,\n        \"min\": -0.5854716356239958,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.5854716356239958,\n          -0.08349138857404798,\n          -0.16535999776800558\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3194198076353616,\n        \"min\": -0.3910311347568131,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.22337523920060878,\n          1.0,\n          0.25446220532709146\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29843375004744976,\n        \"min\": -0.3117007343731907,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2003145523377015,\n          0.13616037988626117,\n          0.07736766291885036\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4251943991536586,\n        \"min\": -0.4575020365957928,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.4435044185350298,\n          -0.3910311347568131,\n          -0.2447981425022257\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce7a0c5-76d6-4863-ba61-0544a220962a",
      "metadata": {
        "id": "8ce7a0c5-76d6-4863-ba61-0544a220962a"
      },
      "source": [
        "#### Converting the DataFrame to a NumPy Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "5735baad-2db8-4306-aa4c-7788d2b49621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5735baad-2db8-4306-aa4c-7788d2b49621",
        "outputId": "d73fb8db-670d-45cf-c269-cc68018282ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(270, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np_data = df.values\n",
        "np_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "29b8e189-7f39-435a-8038-39098b147325",
      "metadata": {
        "id": "29b8e189-7f39-435a-8038-39098b147325"
      },
      "outputs": [],
      "source": [
        "split_point = int(np_data.shape[0]*0.7)\n",
        "\n",
        "np.random.shuffle(np_data)\n",
        "\n",
        "x_train = np_data[:split_point, :13]\n",
        "y_train = np_data[:split_point, 13]\n",
        "x_val = np_data[split_point:, :13]\n",
        "y_val = np_data[split_point:, 13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "fedb56d7-1665-4c90-9697-b86cab43f300",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fedb56d7-1665-4c90-9697-b86cab43f300",
        "outputId": "fe7bde23-6c3c-4d84-c4e4-0d1effb900be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in train and validation are 189 and 81.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train = torch.from_numpy(x_train).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "\n",
        "x_val = torch.from_numpy(x_val).float()\n",
        "y_val = torch.from_numpy(y_val).long()\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f'Number of samples in train and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffc26b9-6044-41e9-93e2-7dc6250dbd27",
      "metadata": {
        "id": "8ffc26b9-6044-41e9-93e2-7dc6250dbd27"
      },
      "source": [
        "## B. Defining Neural Networks\n",
        "\n",
        "In PyTorch, we can use **class** to define our custom neural network architectures by subclassing the `nn.Module` class. This gives our neural network all the functionality it needs to work with PyTorch's other utilities and keeps our implementation organized.\n",
        "\n",
        "- Neural networks are defined by subclassing `nn.Module`.\n",
        "- The layers of the neural network are initialized in the `__init__` method.\n",
        "- The forward pass operations on input data are defined in the `forward` method.\n",
        "\n",
        "It's worth noting that while we only define the forward pass, PyTorch will automatically derive the backward pass for us, which is used during training to update the model's weights.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "77975746-a7a7-4676-9527-57674cd98c0f",
      "metadata": {
        "id": "77975746-a7a7-4676-9527-57674cd98c0f"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class Model( nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(13, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),  # 新增的層\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2)\n",
        "        ).cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb8b5b0-0ec0-406c-a42e-048aa00e05aa",
      "metadata": {
        "id": "cbb8b5b0-0ec0-406c-a42e-048aa00e05aa"
      },
      "source": [
        "## C. Training the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "3602ae7d-4034-4c49-b221-0c12a5824b18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3602ae7d-4034-4c49-b221-0c12a5824b18",
        "outputId": "7845f94d-16ed-4028-a7fd-197b9969605e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 19 12:37:34 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check your GPU status.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "f73a5c35-c15d-49bb-8a33-a7f017159499",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4bb5389311ce4fd18ebbcfec2face2b3",
            "15a45c6a74df4e759878aedba3f25825",
            "bc3424be552a4d3fb442d365d6723d0e",
            "3ebf3ecdab6949e0a35442a7ac5348ff",
            "688ff71d25e14d7481ee90ffa138ea99",
            "1fed1353f5f247ba9f363eaaef6ba785",
            "855973f826574a90b312fd17631cbfd9",
            "1a7d10743e054b1e82db5d2d5a07581a",
            "96a9f4febc1143f692d261370d5f80f5",
            "5d8f94edac58470fb0fcdfa4c41891dc",
            "7d2eec2434f44fbfb77b511469b19fa3"
          ]
        },
        "id": "f73a5c35-c15d-49bb-8a33-a7f017159499",
        "outputId": "47e7545a-48e5-4201-bc09-73d49b3a7403"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bb5389311ce4fd18ebbcfec2face2b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train loss: 5.8361, Train acc: 50.7937%, Val loss: 5.0573, Val acc: 40.7407%, Best Val loss: 5.0573 Best Val acc: 40.74%\n",
            "Epoch 2/100, Train loss: 2.4990, Train acc: 53.4392%, Val loss: 0.6945, Val acc: 59.2593%, Best Val loss: 0.6945 Best Val acc: 59.26%\n",
            "Epoch 3/100, Train loss: 0.9362, Train acc: 57.6720%, Val loss: 0.6719, Val acc: 60.4938%, Best Val loss: 0.6719 Best Val acc: 60.49%\n",
            "Epoch 4/100, Train loss: 0.7745, Train acc: 57.1429%, Val loss: 0.8068, Val acc: 50.6173%, Best Val loss: 0.6719 Best Val acc: 60.49%\n",
            "Epoch 5/100, Train loss: 0.7150, Train acc: 55.5556%, Val loss: 0.5363, Val acc: 69.1358%, Best Val loss: 0.5363 Best Val acc: 69.14%\n",
            "Epoch 6/100, Train loss: 0.6107, Train acc: 65.6085%, Val loss: 0.5256, Val acc: 69.1358%, Best Val loss: 0.5256 Best Val acc: 69.14%\n",
            "Epoch 7/100, Train loss: 0.5925, Train acc: 67.7249%, Val loss: 0.5499, Val acc: 71.6049%, Best Val loss: 0.5256 Best Val acc: 71.60%\n",
            "Epoch 8/100, Train loss: 0.6406, Train acc: 64.5503%, Val loss: 0.5744, Val acc: 65.4321%, Best Val loss: 0.5256 Best Val acc: 71.60%\n",
            "Epoch 9/100, Train loss: 0.6164, Train acc: 67.1958%, Val loss: 0.6078, Val acc: 66.6667%, Best Val loss: 0.5256 Best Val acc: 71.60%\n",
            "Epoch 10/100, Train loss: 0.6114, Train acc: 67.1958%, Val loss: 0.5288, Val acc: 72.8395%, Best Val loss: 0.5256 Best Val acc: 72.84%\n",
            "Epoch 11/100, Train loss: 0.5972, Train acc: 67.7249%, Val loss: 0.5169, Val acc: 69.1358%, Best Val loss: 0.5169 Best Val acc: 72.84%\n",
            "Epoch 12/100, Train loss: 0.5596, Train acc: 71.4286%, Val loss: 0.6671, Val acc: 62.9630%, Best Val loss: 0.5169 Best Val acc: 72.84%\n",
            "Epoch 13/100, Train loss: 0.6409, Train acc: 64.0212%, Val loss: 0.5513, Val acc: 65.4321%, Best Val loss: 0.5169 Best Val acc: 72.84%\n",
            "Epoch 14/100, Train loss: 0.5967, Train acc: 67.1958%, Val loss: 0.5273, Val acc: 72.8395%, Best Val loss: 0.5169 Best Val acc: 72.84%\n",
            "Epoch 15/100, Train loss: 0.5754, Train acc: 68.2540%, Val loss: 0.5197, Val acc: 72.8395%, Best Val loss: 0.5169 Best Val acc: 72.84%\n",
            "Epoch 16/100, Train loss: 0.5747, Train acc: 70.3704%, Val loss: 0.5181, Val acc: 72.8395%, Best Val loss: 0.5169 Best Val acc: 72.84%\n",
            "Epoch 17/100, Train loss: 0.5634, Train acc: 70.3704%, Val loss: 0.5160, Val acc: 72.8395%, Best Val loss: 0.5160 Best Val acc: 72.84%\n",
            "Epoch 18/100, Train loss: 0.5550, Train acc: 71.9577%, Val loss: 0.5609, Val acc: 70.3704%, Best Val loss: 0.5160 Best Val acc: 72.84%\n",
            "Epoch 19/100, Train loss: 0.5613, Train acc: 71.9577%, Val loss: 0.5245, Val acc: 72.8395%, Best Val loss: 0.5160 Best Val acc: 72.84%\n",
            "Epoch 20/100, Train loss: 0.5971, Train acc: 67.7249%, Val loss: 0.5264, Val acc: 71.6049%, Best Val loss: 0.5160 Best Val acc: 72.84%\n",
            "Epoch 21/100, Train loss: 0.5815, Train acc: 70.3704%, Val loss: 0.5700, Val acc: 69.1358%, Best Val loss: 0.5160 Best Val acc: 72.84%\n",
            "Epoch 22/100, Train loss: 0.5532, Train acc: 71.9577%, Val loss: 0.5074, Val acc: 71.6049%, Best Val loss: 0.5074 Best Val acc: 72.84%\n",
            "Epoch 23/100, Train loss: 0.5652, Train acc: 68.7831%, Val loss: 0.5928, Val acc: 66.6667%, Best Val loss: 0.5074 Best Val acc: 72.84%\n",
            "Epoch 24/100, Train loss: 0.5787, Train acc: 67.1958%, Val loss: 0.5300, Val acc: 69.1358%, Best Val loss: 0.5074 Best Val acc: 72.84%\n",
            "Epoch 25/100, Train loss: 0.5965, Train acc: 67.1958%, Val loss: 0.5895, Val acc: 65.4321%, Best Val loss: 0.5074 Best Val acc: 72.84%\n",
            "Epoch 26/100, Train loss: 0.5814, Train acc: 69.8413%, Val loss: 0.5328, Val acc: 71.6049%, Best Val loss: 0.5074 Best Val acc: 72.84%\n",
            "Epoch 27/100, Train loss: 0.5507, Train acc: 71.9577%, Val loss: 0.5091, Val acc: 71.6049%, Best Val loss: 0.5074 Best Val acc: 72.84%\n",
            "Epoch 28/100, Train loss: 0.5472, Train acc: 71.9577%, Val loss: 0.5484, Val acc: 70.3704%, Best Val loss: 0.5074 Best Val acc: 72.84%\n",
            "Epoch 29/100, Train loss: 0.5638, Train acc: 70.8995%, Val loss: 0.5093, Val acc: 70.3704%, Best Val loss: 0.5074 Best Val acc: 72.84%\n",
            "Epoch 30/100, Train loss: 0.5730, Train acc: 69.3122%, Val loss: 0.5587, Val acc: 69.1358%, Best Val loss: 0.5074 Best Val acc: 72.84%\n",
            "Epoch 31/100, Train loss: 0.5456, Train acc: 73.0159%, Val loss: 0.4979, Val acc: 70.3704%, Best Val loss: 0.4979 Best Val acc: 72.84%\n",
            "Epoch 32/100, Train loss: 0.5384, Train acc: 75.1323%, Val loss: 0.5160, Val acc: 74.0741%, Best Val loss: 0.4979 Best Val acc: 74.07%\n",
            "Epoch 33/100, Train loss: 0.5317, Train acc: 73.0159%, Val loss: 0.4977, Val acc: 70.3704%, Best Val loss: 0.4977 Best Val acc: 74.07%\n",
            "Epoch 34/100, Train loss: 0.5394, Train acc: 70.8995%, Val loss: 0.5315, Val acc: 72.8395%, Best Val loss: 0.4977 Best Val acc: 74.07%\n",
            "Epoch 35/100, Train loss: 0.5291, Train acc: 74.0741%, Val loss: 0.4936, Val acc: 70.3704%, Best Val loss: 0.4936 Best Val acc: 74.07%\n",
            "Epoch 36/100, Train loss: 0.5365, Train acc: 73.5450%, Val loss: 0.5453, Val acc: 70.3704%, Best Val loss: 0.4936 Best Val acc: 74.07%\n",
            "Epoch 37/100, Train loss: 0.5371, Train acc: 73.5450%, Val loss: 0.4956, Val acc: 72.8395%, Best Val loss: 0.4936 Best Val acc: 74.07%\n",
            "Epoch 38/100, Train loss: 0.5196, Train acc: 74.0741%, Val loss: 0.5160, Val acc: 74.0741%, Best Val loss: 0.4936 Best Val acc: 74.07%\n",
            "Epoch 39/100, Train loss: 0.5161, Train acc: 73.5450%, Val loss: 0.4895, Val acc: 71.6049%, Best Val loss: 0.4895 Best Val acc: 74.07%\n",
            "Epoch 40/100, Train loss: 0.5373, Train acc: 70.8995%, Val loss: 0.5946, Val acc: 66.6667%, Best Val loss: 0.4895 Best Val acc: 74.07%\n",
            "Epoch 41/100, Train loss: 0.5971, Train acc: 68.2540%, Val loss: 0.4908, Val acc: 69.1358%, Best Val loss: 0.4895 Best Val acc: 74.07%\n",
            "Epoch 42/100, Train loss: 0.5700, Train acc: 68.2540%, Val loss: 0.4957, Val acc: 71.6049%, Best Val loss: 0.4895 Best Val acc: 74.07%\n",
            "Epoch 43/100, Train loss: 0.5379, Train acc: 76.1905%, Val loss: 0.5077, Val acc: 75.3086%, Best Val loss: 0.4895 Best Val acc: 75.31%\n",
            "Epoch 44/100, Train loss: 0.5221, Train acc: 74.6032%, Val loss: 0.4994, Val acc: 72.8395%, Best Val loss: 0.4895 Best Val acc: 75.31%\n",
            "Epoch 45/100, Train loss: 0.5323, Train acc: 72.4868%, Val loss: 0.4852, Val acc: 71.6049%, Best Val loss: 0.4852 Best Val acc: 75.31%\n",
            "Epoch 46/100, Train loss: 0.5239, Train acc: 76.1905%, Val loss: 0.5049, Val acc: 74.0741%, Best Val loss: 0.4852 Best Val acc: 75.31%\n",
            "Epoch 47/100, Train loss: 0.5083, Train acc: 74.0741%, Val loss: 0.4818, Val acc: 70.3704%, Best Val loss: 0.4818 Best Val acc: 75.31%\n",
            "Epoch 48/100, Train loss: 0.5179, Train acc: 70.8995%, Val loss: 0.5172, Val acc: 72.8395%, Best Val loss: 0.4818 Best Val acc: 75.31%\n",
            "Epoch 49/100, Train loss: 0.5497, Train acc: 70.8995%, Val loss: 0.5234, Val acc: 70.3704%, Best Val loss: 0.4818 Best Val acc: 75.31%\n",
            "Epoch 50/100, Train loss: 0.6134, Train acc: 67.1958%, Val loss: 0.4939, Val acc: 74.0741%, Best Val loss: 0.4818 Best Val acc: 75.31%\n",
            "Epoch 51/100, Train loss: 0.5817, Train acc: 69.8413%, Val loss: 0.5203, Val acc: 71.6049%, Best Val loss: 0.4818 Best Val acc: 75.31%\n",
            "Epoch 52/100, Train loss: 0.5477, Train acc: 71.9577%, Val loss: 0.4875, Val acc: 72.8395%, Best Val loss: 0.4818 Best Val acc: 75.31%\n",
            "Epoch 53/100, Train loss: 0.5322, Train acc: 70.3704%, Val loss: 0.4900, Val acc: 72.8395%, Best Val loss: 0.4818 Best Val acc: 75.31%\n",
            "Epoch 54/100, Train loss: 0.5363, Train acc: 69.8413%, Val loss: 0.4979, Val acc: 74.0741%, Best Val loss: 0.4818 Best Val acc: 75.31%\n",
            "Epoch 55/100, Train loss: 0.5286, Train acc: 71.4286%, Val loss: 0.4836, Val acc: 72.8395%, Best Val loss: 0.4818 Best Val acc: 75.31%\n",
            "Epoch 56/100, Train loss: 0.4933, Train acc: 75.1323%, Val loss: 0.5232, Val acc: 71.6049%, Best Val loss: 0.4818 Best Val acc: 75.31%\n",
            "Epoch 57/100, Train loss: 0.5040, Train acc: 73.5450%, Val loss: 0.4791, Val acc: 71.6049%, Best Val loss: 0.4791 Best Val acc: 75.31%\n",
            "Epoch 58/100, Train loss: 0.4978, Train acc: 75.6614%, Val loss: 0.4995, Val acc: 72.8395%, Best Val loss: 0.4791 Best Val acc: 75.31%\n",
            "Epoch 59/100, Train loss: 0.4918, Train acc: 74.0741%, Val loss: 0.4775, Val acc: 71.6049%, Best Val loss: 0.4775 Best Val acc: 75.31%\n",
            "Epoch 60/100, Train loss: 0.4989, Train acc: 77.2487%, Val loss: 0.4897, Val acc: 72.8395%, Best Val loss: 0.4775 Best Val acc: 75.31%\n",
            "Epoch 61/100, Train loss: 0.4896, Train acc: 76.7196%, Val loss: 0.4868, Val acc: 72.8395%, Best Val loss: 0.4775 Best Val acc: 75.31%\n",
            "Epoch 62/100, Train loss: 0.5062, Train acc: 74.6032%, Val loss: 0.4772, Val acc: 71.6049%, Best Val loss: 0.4772 Best Val acc: 75.31%\n",
            "Epoch 63/100, Train loss: 0.5198, Train acc: 70.8995%, Val loss: 0.4934, Val acc: 74.0741%, Best Val loss: 0.4772 Best Val acc: 75.31%\n",
            "Epoch 64/100, Train loss: 0.4918, Train acc: 76.7196%, Val loss: 0.4732, Val acc: 71.6049%, Best Val loss: 0.4732 Best Val acc: 75.31%\n",
            "Epoch 65/100, Train loss: 0.4835, Train acc: 76.7196%, Val loss: 0.4950, Val acc: 74.0741%, Best Val loss: 0.4732 Best Val acc: 75.31%\n",
            "Epoch 66/100, Train loss: 0.4878, Train acc: 74.6032%, Val loss: 0.4801, Val acc: 74.0741%, Best Val loss: 0.4732 Best Val acc: 75.31%\n",
            "Epoch 67/100, Train loss: 0.4797, Train acc: 76.7196%, Val loss: 0.4718, Val acc: 71.6049%, Best Val loss: 0.4718 Best Val acc: 75.31%\n",
            "Epoch 68/100, Train loss: 0.4822, Train acc: 75.1323%, Val loss: 0.4838, Val acc: 75.3086%, Best Val loss: 0.4718 Best Val acc: 75.31%\n",
            "Epoch 69/100, Train loss: 0.4734, Train acc: 77.7778%, Val loss: 0.4727, Val acc: 71.6049%, Best Val loss: 0.4718 Best Val acc: 75.31%\n",
            "Epoch 70/100, Train loss: 0.4734, Train acc: 75.6614%, Val loss: 0.4813, Val acc: 75.3086%, Best Val loss: 0.4718 Best Val acc: 75.31%\n",
            "Epoch 71/100, Train loss: 0.4789, Train acc: 76.1905%, Val loss: 0.4768, Val acc: 74.0741%, Best Val loss: 0.4718 Best Val acc: 75.31%\n",
            "Epoch 72/100, Train loss: 0.4687, Train acc: 77.7778%, Val loss: 0.4658, Val acc: 69.1358%, Best Val loss: 0.4658 Best Val acc: 75.31%\n",
            "Epoch 73/100, Train loss: 0.4721, Train acc: 78.3069%, Val loss: 0.4936, Val acc: 72.8395%, Best Val loss: 0.4658 Best Val acc: 75.31%\n",
            "Epoch 74/100, Train loss: 0.4701, Train acc: 76.7196%, Val loss: 0.4766, Val acc: 74.0741%, Best Val loss: 0.4658 Best Val acc: 75.31%\n",
            "Epoch 75/100, Train loss: 0.4647, Train acc: 74.6032%, Val loss: 0.4668, Val acc: 71.6049%, Best Val loss: 0.4658 Best Val acc: 75.31%\n",
            "Epoch 76/100, Train loss: 0.4639, Train acc: 76.1905%, Val loss: 0.4822, Val acc: 74.0741%, Best Val loss: 0.4658 Best Val acc: 75.31%\n",
            "Epoch 77/100, Train loss: 0.4651, Train acc: 77.2487%, Val loss: 0.4719, Val acc: 74.0741%, Best Val loss: 0.4658 Best Val acc: 75.31%\n",
            "Epoch 78/100, Train loss: 0.4642, Train acc: 77.7778%, Val loss: 0.4787, Val acc: 75.3086%, Best Val loss: 0.4658 Best Val acc: 75.31%\n",
            "Epoch 79/100, Train loss: 0.4669, Train acc: 77.7778%, Val loss: 0.4624, Val acc: 69.1358%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 80/100, Train loss: 0.4683, Train acc: 78.8360%, Val loss: 0.4824, Val acc: 75.3086%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 81/100, Train loss: 0.4581, Train acc: 77.7778%, Val loss: 0.4691, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 82/100, Train loss: 0.4560, Train acc: 77.7778%, Val loss: 0.4681, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 83/100, Train loss: 0.4542, Train acc: 77.7778%, Val loss: 0.4668, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 84/100, Train loss: 0.4515, Train acc: 78.3069%, Val loss: 0.4745, Val acc: 74.0741%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 85/100, Train loss: 0.4520, Train acc: 77.2487%, Val loss: 0.4772, Val acc: 74.0741%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 86/100, Train loss: 0.4515, Train acc: 76.7196%, Val loss: 0.4718, Val acc: 74.0741%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 87/100, Train loss: 0.4543, Train acc: 77.2487%, Val loss: 0.4673, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 88/100, Train loss: 0.4467, Train acc: 77.2487%, Val loss: 0.4698, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 89/100, Train loss: 0.4492, Train acc: 77.7778%, Val loss: 0.4714, Val acc: 74.0741%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 90/100, Train loss: 0.4482, Train acc: 77.2487%, Val loss: 0.4708, Val acc: 74.0741%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 91/100, Train loss: 0.4471, Train acc: 77.7778%, Val loss: 0.4693, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 92/100, Train loss: 0.4482, Train acc: 77.2487%, Val loss: 0.4689, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 93/100, Train loss: 0.4464, Train acc: 77.2487%, Val loss: 0.4687, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 94/100, Train loss: 0.4461, Train acc: 77.2487%, Val loss: 0.4682, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 95/100, Train loss: 0.4476, Train acc: 77.2487%, Val loss: 0.4696, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 96/100, Train loss: 0.4466, Train acc: 77.2487%, Val loss: 0.4693, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 97/100, Train loss: 0.4457, Train acc: 77.2487%, Val loss: 0.4690, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 98/100, Train loss: 0.4450, Train acc: 77.2487%, Val loss: 0.4688, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 99/100, Train loss: 0.4458, Train acc: 77.2487%, Val loss: 0.4686, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n",
            "Epoch 100/100, Train loss: 0.4445, Train acc: 77.2487%, Val loss: 0.4686, Val acc: 72.8395%, Best Val loss: 0.4624 Best Val acc: 75.31%\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "model = Model ()\n",
        "# print(model)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_val_acc = -1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    # Training\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    train_correct = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for features, labels in train_loader:\n",
        "        features = features.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = model(features)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_predicted = outputs.argmax(-1)\n",
        "        train_correct += (train_predicted == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    # Learning rate update\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_accuracy = 100. * train_correct / total_train_samples\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for features, labels in val_loader:\n",
        "            features = features.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = model(features)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            predicted = outputs.argmax(-1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_accuracy = 100. * correct / total\n",
        "\n",
        "    # Checkpoint\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "\n",
        "    if val_accuracy > best_val_acc:\n",
        "        best_val_acc = val_accuracy\n",
        "        torch.save(model.state_dict(), 'model_classification.pth')\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.4f}%, Val loss: {avg_val_loss:.4f}, Val acc: {val_accuracy:.4f}%, Best Val loss: {best_val_loss:.4f} Best Val acc: {best_val_acc:.2f}%')\n",
        "\n",
        "    # Store performance\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7984c6e-6652-4160-b572-07d48bc93a3f",
      "metadata": {
        "id": "a7984c6e-6652-4160-b572-07d48bc93a3f"
      },
      "source": [
        "#### Visualizing the model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "5559d850-1fb5-4b04-b6ca-60c5b309f34e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "5559d850-1fb5-4b04-b6ca-60c5b309f34e",
        "outputId": "4ff05a7f-dba5-428f-8205-ee788ce02bc1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "ax[0].plot(train_accuracies)\n",
        "ax[0].plot(val_accuracies)\n",
        "ax[0].set_title('Model Accuracy')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].legend(['Train', 'Val'])\n",
        "\n",
        "# Plotting training and validation loss\n",
        "ax[1].plot(train_losses)\n",
        "ax[1].plot(val_losses)\n",
        "ax[1].set_title('Model Loss')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].legend(['Train', 'Val'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89c7e51b-8ab6-4aa2-877d-39b6daf55c20",
      "metadata": {
        "id": "89c7e51b-8ab6-4aa2-877d-39b6daf55c20"
      },
      "source": [
        "## D. Evaluating Your Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "f49735d7-466f-4037-8078-172f03dffd8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f49735d7-466f-4037-8078-172f03dffd8d",
        "outputId": "533ac630-eab8-4543-d670-e25c646f7217"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   50    0   2       120   219    0        1      158      0      1.6      1   \n",
              "1   58    0   2       120   340    0        1      172      0      0.0      2   \n",
              "2   66    0   3       150   226    0        1      114      0      2.6      0   \n",
              "3   43    1   0       150   247    0        1      171      0      1.5      2   \n",
              "4   69    0   3       140   239    0        1      151      0      1.8      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     2       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   2     2       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57c7956d-30f9-48fe-9d5f-d1d1fca71dcc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>219</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>158</td>\n",
              "      <td>0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>120</td>\n",
              "      <td>340</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>150</td>\n",
              "      <td>226</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>114</td>\n",
              "      <td>0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>247</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>140</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57c7956d-30f9-48fe-9d5f-d1d1fca71dcc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57c7956d-30f9-48fe-9d5f-d1d1fca71dcc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57c7956d-30f9-48fe-9d5f-d1d1fca71dcc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f2d1d496-4854-4b6e-b73b-3ce921a886f6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f2d1d496-4854-4b6e-b73b-3ce921a886f6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f2d1d496-4854-4b6e-b73b-3ce921a886f6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_data",
              "summary": "{\n  \"name\": \"test_data\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 40,\n        \"max\": 71,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          50,\n          58,\n          61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trestbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 110,\n        \"max\": 170,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          145,\n          124\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 172,\n        \"max\": 417,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          302,\n          282\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fbs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"restecg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thalach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 109,\n        \"max\": 179,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          137,\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exang\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"oldpeak\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.976872340632323,\n        \"min\": 0.0,\n        \"max\": 3.4,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1.6,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ca\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"thal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# read test file\n",
        "test_data = pd.read_csv('heart_dataset_test.csv')\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "21ae9d85-0dc2-4db0-a7c7-807c6b6c514f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ae9d85-0dc2-4db0-a7c7-807c6b6c514f",
        "outputId": "4ced34de-99d2-4deb-a7b0-08bc9b281835"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "cp          0\n",
              "trestbps    0\n",
              "chol        0\n",
              "fbs         0\n",
              "restecg     0\n",
              "thalach     0\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       0\n",
              "ca          0\n",
              "thal        0\n",
              "target      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "test_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "5ff2812b-a5a5-4ea9-86be-ae2143cb2ba7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ff2812b-a5a5-4ea9-86be-ae2143cb2ba7",
        "outputId": "6f136591-33c0-46a0-e965-e72c67defa0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "test_data = test_data.values\n",
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "14d4be20-f64f-421d-8971-e1e47873aef8",
      "metadata": {
        "id": "14d4be20-f64f-421d-8971-e1e47873aef8"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "x_test = torch.from_numpy(test_data[:, :13]).float()\n",
        "y_test = torch.from_numpy(test_data[:, 13]).long()\n",
        "\n",
        "# Create datasets\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "# Create dataloaders\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "4bcf8580-42ee-4ee7-ad15-9f080cc57a33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bcf8580-42ee-4ee7-ad15-9f080cc57a33",
        "outputId": "254e49a6-6bd1-4ad7-eba9-6ed634fbd2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is 64.51612903225806%\n"
          ]
        }
      ],
      "source": [
        "# Load the trained weights\n",
        "model.load_state_dict(torch.load('model_classification.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "\n",
        "        features = features.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = model(features)\n",
        "\n",
        "        predicted = outputs.argmax(-1)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "print(f'Test accuracy is {100. * test_correct / test_total}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 模型定義\n",
        "class ModelPlus1Layer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(13, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),  # 新增的層\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2)\n",
        "        ).cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 訓練和驗證函數\n",
        "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # 訓練階段\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # 驗證階段\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = correct / total\n",
        "\n",
        "        # 更新最佳驗證結果\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "        print(f'Epoch: {epoch+1}/{epochs}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}, Best Val loss: {best_val_loss:.4f}, Best Val acc: {best_val_acc:.4f}')\n",
        "\n",
        "# 模型、優化器和損失函數的實例化\n",
        "model = ModelPlus1Layer()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 假定的訓練和驗證數據集\n",
        "# train_dataset = ...\n",
        "# val_dataset = ...\n",
        "\n",
        "# 訓練參數\n",
        "batch_sizes = [16, 32, 64]\n",
        "epoch_settings = [90, 100, 110]\n",
        "\n",
        "# 訓練循環\n",
        "for batch_size in batch_sizes:\n",
        "    for epochs in epoch_settings:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "        print(f'Training with batch size: {batch_size}, epochs: {epochs}')\n",
        "        train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
        "def test_accuracy(model, test_loader):\n",
        "    model.eval()  # 將模型設置為評估模式\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # 不計算梯度\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# 現有的訓練和驗證函數 ...\n",
        "# ... (您之前的代碼) ...\n",
        "\n",
        "# 假定的測試數據集\n",
        "# test_dataset = ...\n",
        "\n",
        "# 訓練循環\n",
        "for batch_size in batch_sizes:\n",
        "    for epochs in epoch_settings:\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size)  # 添加測試數據加載器\n",
        "\n",
        "        print(f'Training with batch size: {batch_size}, epochs: {epochs}')\n",
        "        train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
        "\n",
        "        test_acc = test_accuracy(model, test_loader)\n",
        "        print(f'Test Accuracy for batch size {batch_size}, epochs {epochs}: {test_acc:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohiN_D4bI8nE",
        "outputId": "faa290eb-742e-4a0d-98d5-c7cb13e79ec2"
      },
      "id": "ohiN_D4bI8nE",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with batch size: 16, epochs: 90\n",
            "Epoch: 1/90, Train loss: 2.5935, Train acc: 0.5450, Val loss: 1.4018, Val acc: 0.5926, Best Val loss: 1.4018, Best Val acc: 0.5926\n",
            "Epoch: 2/90, Train loss: 0.7444, Train acc: 0.6032, Val loss: 0.5415, Val acc: 0.7284, Best Val loss: 0.5415, Best Val acc: 0.7284\n",
            "Epoch: 3/90, Train loss: 0.6376, Train acc: 0.6085, Val loss: 0.6243, Val acc: 0.6296, Best Val loss: 0.5415, Best Val acc: 0.7284\n",
            "Epoch: 4/90, Train loss: 0.6144, Train acc: 0.6614, Val loss: 0.5317, Val acc: 0.7037, Best Val loss: 0.5317, Best Val acc: 0.7037\n",
            "Epoch: 5/90, Train loss: 0.6094, Train acc: 0.6720, Val loss: 0.5450, Val acc: 0.6790, Best Val loss: 0.5317, Best Val acc: 0.7037\n",
            "Epoch: 6/90, Train loss: 0.6177, Train acc: 0.6296, Val loss: 0.5283, Val acc: 0.6914, Best Val loss: 0.5283, Best Val acc: 0.6914\n",
            "Epoch: 7/90, Train loss: 0.6003, Train acc: 0.6614, Val loss: 0.5243, Val acc: 0.7160, Best Val loss: 0.5243, Best Val acc: 0.7160\n",
            "Epoch: 8/90, Train loss: 0.6136, Train acc: 0.7090, Val loss: 0.5448, Val acc: 0.7284, Best Val loss: 0.5243, Best Val acc: 0.7160\n",
            "Epoch: 9/90, Train loss: 0.5806, Train acc: 0.6984, Val loss: 0.5461, Val acc: 0.7037, Best Val loss: 0.5243, Best Val acc: 0.7160\n",
            "Epoch: 10/90, Train loss: 0.5653, Train acc: 0.6878, Val loss: 0.5213, Val acc: 0.7284, Best Val loss: 0.5213, Best Val acc: 0.7284\n",
            "Epoch: 11/90, Train loss: 0.5418, Train acc: 0.7037, Val loss: 0.5474, Val acc: 0.6667, Best Val loss: 0.5213, Best Val acc: 0.7284\n",
            "Epoch: 12/90, Train loss: 0.5928, Train acc: 0.6296, Val loss: 0.5288, Val acc: 0.6914, Best Val loss: 0.5213, Best Val acc: 0.7284\n",
            "Epoch: 13/90, Train loss: 0.5585, Train acc: 0.6825, Val loss: 0.6523, Val acc: 0.6790, Best Val loss: 0.5213, Best Val acc: 0.7284\n",
            "Epoch: 14/90, Train loss: 0.6350, Train acc: 0.6402, Val loss: 0.5328, Val acc: 0.6914, Best Val loss: 0.5213, Best Val acc: 0.7284\n",
            "Epoch: 15/90, Train loss: 0.5637, Train acc: 0.7037, Val loss: 0.5136, Val acc: 0.7160, Best Val loss: 0.5136, Best Val acc: 0.7160\n",
            "Epoch: 16/90, Train loss: 0.5513, Train acc: 0.7354, Val loss: 0.5167, Val acc: 0.7284, Best Val loss: 0.5136, Best Val acc: 0.7160\n",
            "Epoch: 17/90, Train loss: 0.5411, Train acc: 0.7354, Val loss: 0.5092, Val acc: 0.6914, Best Val loss: 0.5092, Best Val acc: 0.6914\n",
            "Epoch: 18/90, Train loss: 0.5443, Train acc: 0.7249, Val loss: 0.5204, Val acc: 0.6914, Best Val loss: 0.5092, Best Val acc: 0.6914\n",
            "Epoch: 19/90, Train loss: 0.5705, Train acc: 0.6720, Val loss: 0.5033, Val acc: 0.7160, Best Val loss: 0.5033, Best Val acc: 0.7160\n",
            "Epoch: 20/90, Train loss: 0.5602, Train acc: 0.6931, Val loss: 0.5155, Val acc: 0.6914, Best Val loss: 0.5033, Best Val acc: 0.7160\n",
            "Epoch: 21/90, Train loss: 0.5355, Train acc: 0.6984, Val loss: 0.5161, Val acc: 0.7160, Best Val loss: 0.5033, Best Val acc: 0.7160\n",
            "Epoch: 22/90, Train loss: 0.5544, Train acc: 0.7249, Val loss: 0.5824, Val acc: 0.6173, Best Val loss: 0.5033, Best Val acc: 0.7160\n",
            "Epoch: 23/90, Train loss: 0.5708, Train acc: 0.7249, Val loss: 0.5663, Val acc: 0.7160, Best Val loss: 0.5033, Best Val acc: 0.7160\n",
            "Epoch: 24/90, Train loss: 0.5527, Train acc: 0.6984, Val loss: 0.6548, Val acc: 0.6790, Best Val loss: 0.5033, Best Val acc: 0.7160\n",
            "Epoch: 25/90, Train loss: 0.6124, Train acc: 0.6878, Val loss: 0.5228, Val acc: 0.7284, Best Val loss: 0.5033, Best Val acc: 0.7160\n",
            "Epoch: 26/90, Train loss: 0.5291, Train acc: 0.7090, Val loss: 0.5403, Val acc: 0.7037, Best Val loss: 0.5033, Best Val acc: 0.7160\n",
            "Epoch: 27/90, Train loss: 0.5558, Train acc: 0.7090, Val loss: 0.4965, Val acc: 0.7160, Best Val loss: 0.4965, Best Val acc: 0.7160\n",
            "Epoch: 28/90, Train loss: 0.5171, Train acc: 0.7302, Val loss: 0.5231, Val acc: 0.7037, Best Val loss: 0.4965, Best Val acc: 0.7160\n",
            "Epoch: 29/90, Train loss: 0.5286, Train acc: 0.7302, Val loss: 0.5055, Val acc: 0.7160, Best Val loss: 0.4965, Best Val acc: 0.7160\n",
            "Epoch: 30/90, Train loss: 0.5640, Train acc: 0.6931, Val loss: 0.5462, Val acc: 0.7778, Best Val loss: 0.4965, Best Val acc: 0.7160\n",
            "Epoch: 31/90, Train loss: 0.5090, Train acc: 0.7302, Val loss: 0.5758, Val acc: 0.6420, Best Val loss: 0.4965, Best Val acc: 0.7160\n",
            "Epoch: 32/90, Train loss: 0.5078, Train acc: 0.7407, Val loss: 0.4823, Val acc: 0.7284, Best Val loss: 0.4823, Best Val acc: 0.7284\n",
            "Epoch: 33/90, Train loss: 0.5135, Train acc: 0.7249, Val loss: 0.4771, Val acc: 0.7284, Best Val loss: 0.4771, Best Val acc: 0.7284\n",
            "Epoch: 34/90, Train loss: 0.4871, Train acc: 0.7513, Val loss: 0.4747, Val acc: 0.7160, Best Val loss: 0.4747, Best Val acc: 0.7160\n",
            "Epoch: 35/90, Train loss: 0.4741, Train acc: 0.7566, Val loss: 0.4675, Val acc: 0.6914, Best Val loss: 0.4675, Best Val acc: 0.6914\n",
            "Epoch: 36/90, Train loss: 0.4713, Train acc: 0.7778, Val loss: 0.4681, Val acc: 0.7284, Best Val loss: 0.4675, Best Val acc: 0.6914\n",
            "Epoch: 37/90, Train loss: 0.4857, Train acc: 0.7460, Val loss: 0.5776, Val acc: 0.7407, Best Val loss: 0.4675, Best Val acc: 0.6914\n",
            "Epoch: 38/90, Train loss: 0.5643, Train acc: 0.6667, Val loss: 0.5533, Val acc: 0.7407, Best Val loss: 0.4675, Best Val acc: 0.6914\n",
            "Epoch: 39/90, Train loss: 0.5042, Train acc: 0.7725, Val loss: 0.4726, Val acc: 0.7654, Best Val loss: 0.4675, Best Val acc: 0.6914\n",
            "Epoch: 40/90, Train loss: 0.5139, Train acc: 0.7513, Val loss: 0.4556, Val acc: 0.7160, Best Val loss: 0.4556, Best Val acc: 0.7160\n",
            "Epoch: 41/90, Train loss: 0.4858, Train acc: 0.7354, Val loss: 0.5010, Val acc: 0.7654, Best Val loss: 0.4556, Best Val acc: 0.7160\n",
            "Epoch: 42/90, Train loss: 0.4625, Train acc: 0.7672, Val loss: 0.4519, Val acc: 0.7531, Best Val loss: 0.4519, Best Val acc: 0.7531\n",
            "Epoch: 43/90, Train loss: 0.4442, Train acc: 0.7725, Val loss: 0.4445, Val acc: 0.7901, Best Val loss: 0.4445, Best Val acc: 0.7901\n",
            "Epoch: 44/90, Train loss: 0.4585, Train acc: 0.7725, Val loss: 0.5770, Val acc: 0.6296, Best Val loss: 0.4445, Best Val acc: 0.7901\n",
            "Epoch: 45/90, Train loss: 0.4364, Train acc: 0.8095, Val loss: 0.5268, Val acc: 0.7037, Best Val loss: 0.4445, Best Val acc: 0.7901\n",
            "Epoch: 46/90, Train loss: 0.4066, Train acc: 0.8360, Val loss: 0.4152, Val acc: 0.7531, Best Val loss: 0.4152, Best Val acc: 0.7531\n",
            "Epoch: 47/90, Train loss: 0.4531, Train acc: 0.7778, Val loss: 0.4173, Val acc: 0.8272, Best Val loss: 0.4152, Best Val acc: 0.7531\n",
            "Epoch: 48/90, Train loss: 0.5512, Train acc: 0.6825, Val loss: 0.6047, Val acc: 0.5926, Best Val loss: 0.4152, Best Val acc: 0.7531\n",
            "Epoch: 49/90, Train loss: 0.5815, Train acc: 0.6825, Val loss: 0.6667, Val acc: 0.5926, Best Val loss: 0.4152, Best Val acc: 0.7531\n",
            "Epoch: 50/90, Train loss: 0.5241, Train acc: 0.7143, Val loss: 0.4234, Val acc: 0.7531, Best Val loss: 0.4152, Best Val acc: 0.7531\n",
            "Epoch: 51/90, Train loss: 0.4275, Train acc: 0.8095, Val loss: 0.4108, Val acc: 0.7901, Best Val loss: 0.4108, Best Val acc: 0.7901\n",
            "Epoch: 52/90, Train loss: 0.4083, Train acc: 0.7989, Val loss: 0.4897, Val acc: 0.7778, Best Val loss: 0.4108, Best Val acc: 0.7901\n",
            "Epoch: 53/90, Train loss: 0.4348, Train acc: 0.8148, Val loss: 0.4029, Val acc: 0.8148, Best Val loss: 0.4029, Best Val acc: 0.8148\n",
            "Epoch: 54/90, Train loss: 0.4081, Train acc: 0.7937, Val loss: 0.4823, Val acc: 0.7160, Best Val loss: 0.4029, Best Val acc: 0.8148\n",
            "Epoch: 55/90, Train loss: 0.4524, Train acc: 0.7989, Val loss: 0.4473, Val acc: 0.7531, Best Val loss: 0.4029, Best Val acc: 0.8148\n",
            "Epoch: 56/90, Train loss: 0.4629, Train acc: 0.7672, Val loss: 0.4248, Val acc: 0.7654, Best Val loss: 0.4029, Best Val acc: 0.8148\n",
            "Epoch: 57/90, Train loss: 0.4646, Train acc: 0.7725, Val loss: 0.5703, Val acc: 0.6420, Best Val loss: 0.4029, Best Val acc: 0.8148\n",
            "Epoch: 58/90, Train loss: 0.4366, Train acc: 0.7989, Val loss: 0.3957, Val acc: 0.8148, Best Val loss: 0.3957, Best Val acc: 0.8148\n",
            "Epoch: 59/90, Train loss: 0.3784, Train acc: 0.8254, Val loss: 0.4912, Val acc: 0.7284, Best Val loss: 0.3957, Best Val acc: 0.8148\n",
            "Epoch: 60/90, Train loss: 0.4307, Train acc: 0.7989, Val loss: 0.4156, Val acc: 0.7654, Best Val loss: 0.3957, Best Val acc: 0.8148\n",
            "Epoch: 61/90, Train loss: 0.3951, Train acc: 0.7778, Val loss: 0.3798, Val acc: 0.8025, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 62/90, Train loss: 0.3730, Train acc: 0.8254, Val loss: 0.4637, Val acc: 0.7654, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 63/90, Train loss: 0.4081, Train acc: 0.7778, Val loss: 0.3935, Val acc: 0.8025, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 64/90, Train loss: 0.3954, Train acc: 0.8360, Val loss: 0.4829, Val acc: 0.7160, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 65/90, Train loss: 0.4425, Train acc: 0.7937, Val loss: 0.5185, Val acc: 0.6790, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 66/90, Train loss: 0.3779, Train acc: 0.8201, Val loss: 0.3958, Val acc: 0.8025, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 67/90, Train loss: 0.3721, Train acc: 0.8466, Val loss: 0.4273, Val acc: 0.7901, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 68/90, Train loss: 0.3431, Train acc: 0.8519, Val loss: 0.4550, Val acc: 0.7778, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 69/90, Train loss: 0.3593, Train acc: 0.8254, Val loss: 0.4333, Val acc: 0.8025, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 70/90, Train loss: 0.3590, Train acc: 0.8201, Val loss: 0.4792, Val acc: 0.7654, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 71/90, Train loss: 0.4418, Train acc: 0.7937, Val loss: 0.4165, Val acc: 0.8272, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 72/90, Train loss: 0.3587, Train acc: 0.8254, Val loss: 0.4210, Val acc: 0.7901, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 73/90, Train loss: 0.3664, Train acc: 0.8254, Val loss: 0.3898, Val acc: 0.8025, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 74/90, Train loss: 0.3646, Train acc: 0.8307, Val loss: 0.3945, Val acc: 0.8148, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 75/90, Train loss: 0.3311, Train acc: 0.8466, Val loss: 0.3910, Val acc: 0.8148, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 76/90, Train loss: 0.3696, Train acc: 0.8413, Val loss: 0.4139, Val acc: 0.7654, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 77/90, Train loss: 0.3840, Train acc: 0.7989, Val loss: 0.4162, Val acc: 0.7778, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 78/90, Train loss: 0.3396, Train acc: 0.8413, Val loss: 0.3984, Val acc: 0.8272, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 79/90, Train loss: 0.3453, Train acc: 0.8466, Val loss: 0.3854, Val acc: 0.8272, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 80/90, Train loss: 0.4077, Train acc: 0.8307, Val loss: 0.4478, Val acc: 0.7284, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 81/90, Train loss: 0.4175, Train acc: 0.8095, Val loss: 0.4826, Val acc: 0.7407, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 82/90, Train loss: 0.3749, Train acc: 0.8042, Val loss: 0.4443, Val acc: 0.7778, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 83/90, Train loss: 0.3507, Train acc: 0.8307, Val loss: 0.3984, Val acc: 0.8272, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 84/90, Train loss: 0.3986, Train acc: 0.8413, Val loss: 0.3889, Val acc: 0.8148, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 85/90, Train loss: 0.4097, Train acc: 0.8042, Val loss: 0.4205, Val acc: 0.8272, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 86/90, Train loss: 0.3534, Train acc: 0.8413, Val loss: 0.4243, Val acc: 0.7901, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 87/90, Train loss: 0.3845, Train acc: 0.8201, Val loss: 0.4242, Val acc: 0.7654, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 88/90, Train loss: 0.4460, Train acc: 0.7778, Val loss: 0.3890, Val acc: 0.8272, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 89/90, Train loss: 0.3967, Train acc: 0.8042, Val loss: 0.4597, Val acc: 0.8148, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Epoch: 90/90, Train loss: 0.3839, Train acc: 0.8360, Val loss: 0.3968, Val acc: 0.8519, Best Val loss: 0.3798, Best Val acc: 0.8025\n",
            "Training with batch size: 16, epochs: 100\n",
            "Epoch: 1/100, Train loss: 0.3404, Train acc: 0.8413, Val loss: 0.4049, Val acc: 0.8272, Best Val loss: 0.4049, Best Val acc: 0.8272\n",
            "Epoch: 2/100, Train loss: 0.3288, Train acc: 0.8466, Val loss: 0.3840, Val acc: 0.8272, Best Val loss: 0.3840, Best Val acc: 0.8272\n",
            "Epoch: 3/100, Train loss: 0.3665, Train acc: 0.8201, Val loss: 0.5428, Val acc: 0.7037, Best Val loss: 0.3840, Best Val acc: 0.8272\n",
            "Epoch: 4/100, Train loss: 0.4529, Train acc: 0.7725, Val loss: 0.4466, Val acc: 0.7654, Best Val loss: 0.3840, Best Val acc: 0.8272\n",
            "Epoch: 5/100, Train loss: 0.3820, Train acc: 0.8307, Val loss: 0.3900, Val acc: 0.8272, Best Val loss: 0.3840, Best Val acc: 0.8272\n",
            "Epoch: 6/100, Train loss: 0.3516, Train acc: 0.8466, Val loss: 0.3741, Val acc: 0.8395, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 7/100, Train loss: 0.3456, Train acc: 0.8519, Val loss: 0.4142, Val acc: 0.7654, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 8/100, Train loss: 0.3453, Train acc: 0.8201, Val loss: 0.4078, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 9/100, Train loss: 0.3423, Train acc: 0.8254, Val loss: 0.4106, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 10/100, Train loss: 0.3299, Train acc: 0.8254, Val loss: 0.4645, Val acc: 0.7778, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 11/100, Train loss: 0.3838, Train acc: 0.8201, Val loss: 0.5834, Val acc: 0.7531, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 12/100, Train loss: 0.3627, Train acc: 0.8360, Val loss: 0.4069, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 13/100, Train loss: 0.3200, Train acc: 0.8624, Val loss: 0.4150, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 14/100, Train loss: 0.3357, Train acc: 0.8360, Val loss: 0.3984, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 15/100, Train loss: 0.3238, Train acc: 0.8413, Val loss: 0.4817, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 16/100, Train loss: 0.3304, Train acc: 0.8360, Val loss: 0.3986, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 17/100, Train loss: 0.3036, Train acc: 0.8360, Val loss: 0.4329, Val acc: 0.7654, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 18/100, Train loss: 0.3153, Train acc: 0.8466, Val loss: 0.4328, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 19/100, Train loss: 0.3045, Train acc: 0.8730, Val loss: 0.4225, Val acc: 0.8395, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 20/100, Train loss: 0.3009, Train acc: 0.8360, Val loss: 0.4191, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 21/100, Train loss: 0.3028, Train acc: 0.8307, Val loss: 0.4332, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 22/100, Train loss: 0.2868, Train acc: 0.8519, Val loss: 0.4298, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 23/100, Train loss: 0.3016, Train acc: 0.8677, Val loss: 0.4392, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 24/100, Train loss: 0.3213, Train acc: 0.8307, Val loss: 0.4336, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 25/100, Train loss: 0.3076, Train acc: 0.8624, Val loss: 0.4467, Val acc: 0.8395, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 26/100, Train loss: 0.3036, Train acc: 0.8624, Val loss: 0.4356, Val acc: 0.7654, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 27/100, Train loss: 0.2957, Train acc: 0.8413, Val loss: 0.5936, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 28/100, Train loss: 0.3199, Train acc: 0.8095, Val loss: 0.4459, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 29/100, Train loss: 0.3036, Train acc: 0.8571, Val loss: 0.4216, Val acc: 0.8519, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 30/100, Train loss: 0.2842, Train acc: 0.8783, Val loss: 0.4342, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 31/100, Train loss: 0.3074, Train acc: 0.8413, Val loss: 0.4450, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 32/100, Train loss: 0.3486, Train acc: 0.8095, Val loss: 0.4219, Val acc: 0.8519, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 33/100, Train loss: 0.3244, Train acc: 0.8360, Val loss: 0.4716, Val acc: 0.7654, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 34/100, Train loss: 0.3759, Train acc: 0.8413, Val loss: 0.4504, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 35/100, Train loss: 0.3132, Train acc: 0.8360, Val loss: 0.4023, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 36/100, Train loss: 0.3038, Train acc: 0.8571, Val loss: 0.4421, Val acc: 0.8395, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 37/100, Train loss: 0.3060, Train acc: 0.8360, Val loss: 0.4854, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 38/100, Train loss: 0.3078, Train acc: 0.8360, Val loss: 0.4566, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 39/100, Train loss: 0.3179, Train acc: 0.8571, Val loss: 0.4309, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 40/100, Train loss: 0.3451, Train acc: 0.7884, Val loss: 0.3996, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 41/100, Train loss: 0.2960, Train acc: 0.8624, Val loss: 0.4848, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 42/100, Train loss: 0.2713, Train acc: 0.8413, Val loss: 0.6535, Val acc: 0.6790, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 43/100, Train loss: 0.3761, Train acc: 0.8466, Val loss: 0.4083, Val acc: 0.7407, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 44/100, Train loss: 0.3210, Train acc: 0.8413, Val loss: 0.4239, Val acc: 0.7531, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 45/100, Train loss: 0.3295, Train acc: 0.8466, Val loss: 0.4370, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 46/100, Train loss: 0.3073, Train acc: 0.8730, Val loss: 0.6605, Val acc: 0.7778, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 47/100, Train loss: 0.2906, Train acc: 0.8519, Val loss: 0.4192, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 48/100, Train loss: 0.2926, Train acc: 0.8624, Val loss: 0.4481, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 49/100, Train loss: 0.3246, Train acc: 0.8413, Val loss: 0.4269, Val acc: 0.8272, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 50/100, Train loss: 0.3702, Train acc: 0.8307, Val loss: 0.4220, Val acc: 0.8395, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 51/100, Train loss: 0.3227, Train acc: 0.8413, Val loss: 0.4681, Val acc: 0.7778, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 52/100, Train loss: 0.2917, Train acc: 0.8624, Val loss: 0.4498, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 53/100, Train loss: 0.2702, Train acc: 0.8730, Val loss: 0.4483, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 54/100, Train loss: 0.2828, Train acc: 0.8519, Val loss: 0.4340, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 55/100, Train loss: 0.3265, Train acc: 0.8519, Val loss: 0.4842, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 56/100, Train loss: 0.3117, Train acc: 0.8254, Val loss: 0.6747, Val acc: 0.7531, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 57/100, Train loss: 0.3482, Train acc: 0.8360, Val loss: 0.4339, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 58/100, Train loss: 0.4304, Train acc: 0.7831, Val loss: 0.4251, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 59/100, Train loss: 0.3603, Train acc: 0.8095, Val loss: 0.4135, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 60/100, Train loss: 0.3167, Train acc: 0.8095, Val loss: 0.4685, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 61/100, Train loss: 0.2845, Train acc: 0.8677, Val loss: 0.4948, Val acc: 0.7778, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 62/100, Train loss: 0.2737, Train acc: 0.8730, Val loss: 0.4465, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 63/100, Train loss: 0.2522, Train acc: 0.8730, Val loss: 0.4826, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 64/100, Train loss: 0.2584, Train acc: 0.8889, Val loss: 0.4913, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 65/100, Train loss: 0.2524, Train acc: 0.8942, Val loss: 0.5084, Val acc: 0.7778, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 66/100, Train loss: 0.2504, Train acc: 0.8783, Val loss: 0.4907, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 67/100, Train loss: 0.2821, Train acc: 0.8360, Val loss: 0.5300, Val acc: 0.7778, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 68/100, Train loss: 0.3157, Train acc: 0.8254, Val loss: 0.5245, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 69/100, Train loss: 0.3321, Train acc: 0.8042, Val loss: 0.4461, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 70/100, Train loss: 0.3712, Train acc: 0.8042, Val loss: 0.4580, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 71/100, Train loss: 0.3305, Train acc: 0.8360, Val loss: 0.4681, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 72/100, Train loss: 0.2817, Train acc: 0.8942, Val loss: 0.4884, Val acc: 0.7778, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 73/100, Train loss: 0.2710, Train acc: 0.8571, Val loss: 0.4807, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 74/100, Train loss: 0.2806, Train acc: 0.8519, Val loss: 0.5346, Val acc: 0.7778, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 75/100, Train loss: 0.3010, Train acc: 0.8148, Val loss: 0.5442, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 76/100, Train loss: 0.2861, Train acc: 0.8413, Val loss: 0.4970, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 77/100, Train loss: 0.2720, Train acc: 0.8519, Val loss: 0.5047, Val acc: 0.7407, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 78/100, Train loss: 0.2764, Train acc: 0.8571, Val loss: 0.4855, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 79/100, Train loss: 0.2530, Train acc: 0.8942, Val loss: 0.5358, Val acc: 0.7531, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 80/100, Train loss: 0.2599, Train acc: 0.8677, Val loss: 0.4914, Val acc: 0.7654, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 81/100, Train loss: 0.2734, Train acc: 0.8730, Val loss: 0.4897, Val acc: 0.7654, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 82/100, Train loss: 0.2791, Train acc: 0.8466, Val loss: 0.5054, Val acc: 0.8272, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 83/100, Train loss: 0.2948, Train acc: 0.8889, Val loss: 0.5255, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 84/100, Train loss: 0.2631, Train acc: 0.8624, Val loss: 0.5072, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 85/100, Train loss: 0.2374, Train acc: 0.8942, Val loss: 0.4825, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 86/100, Train loss: 0.2341, Train acc: 0.8783, Val loss: 0.5208, Val acc: 0.8272, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 87/100, Train loss: 0.2237, Train acc: 0.9048, Val loss: 0.5455, Val acc: 0.7407, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 88/100, Train loss: 0.2348, Train acc: 0.8783, Val loss: 0.5417, Val acc: 0.7778, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 89/100, Train loss: 0.2945, Train acc: 0.8783, Val loss: 0.4754, Val acc: 0.7654, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 90/100, Train loss: 0.3664, Train acc: 0.8466, Val loss: 0.4345, Val acc: 0.8272, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 91/100, Train loss: 0.3219, Train acc: 0.8201, Val loss: 0.4149, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 92/100, Train loss: 0.3017, Train acc: 0.8519, Val loss: 0.5052, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 93/100, Train loss: 0.2482, Train acc: 0.8730, Val loss: 0.5805, Val acc: 0.8025, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 94/100, Train loss: 0.2597, Train acc: 0.8624, Val loss: 0.5271, Val acc: 0.7654, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 95/100, Train loss: 0.2943, Train acc: 0.8519, Val loss: 0.5782, Val acc: 0.7901, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 96/100, Train loss: 0.2607, Train acc: 0.8889, Val loss: 0.4821, Val acc: 0.7778, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 97/100, Train loss: 0.2306, Train acc: 0.8995, Val loss: 0.5482, Val acc: 0.8148, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 98/100, Train loss: 0.2425, Train acc: 0.8783, Val loss: 0.4976, Val acc: 0.7531, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 99/100, Train loss: 0.2339, Train acc: 0.8730, Val loss: 0.5452, Val acc: 0.7160, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Epoch: 100/100, Train loss: 0.2478, Train acc: 0.8730, Val loss: 0.5108, Val acc: 0.7284, Best Val loss: 0.3741, Best Val acc: 0.8395\n",
            "Training with batch size: 16, epochs: 110\n",
            "Epoch: 1/110, Train loss: 0.2615, Train acc: 0.8624, Val loss: 0.4650, Val acc: 0.7654, Best Val loss: 0.4650, Best Val acc: 0.7654\n",
            "Epoch: 2/110, Train loss: 0.2308, Train acc: 0.8995, Val loss: 0.4955, Val acc: 0.7901, Best Val loss: 0.4650, Best Val acc: 0.7654\n",
            "Epoch: 3/110, Train loss: 0.3205, Train acc: 0.8413, Val loss: 0.5308, Val acc: 0.7284, Best Val loss: 0.4650, Best Val acc: 0.7654\n",
            "Epoch: 4/110, Train loss: 0.4316, Train acc: 0.7884, Val loss: 0.4586, Val acc: 0.7654, Best Val loss: 0.4586, Best Val acc: 0.7654\n",
            "Epoch: 5/110, Train loss: 0.3927, Train acc: 0.7725, Val loss: 0.5441, Val acc: 0.7654, Best Val loss: 0.4586, Best Val acc: 0.7654\n",
            "Epoch: 6/110, Train loss: 0.3688, Train acc: 0.8148, Val loss: 0.4516, Val acc: 0.7901, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 7/110, Train loss: 0.3066, Train acc: 0.8201, Val loss: 0.4706, Val acc: 0.8148, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 8/110, Train loss: 0.3281, Train acc: 0.7778, Val loss: 0.5299, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 9/110, Train loss: 0.2841, Train acc: 0.8413, Val loss: 0.4556, Val acc: 0.8148, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 10/110, Train loss: 0.2730, Train acc: 0.8413, Val loss: 0.5036, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 11/110, Train loss: 0.2744, Train acc: 0.8307, Val loss: 0.4810, Val acc: 0.7531, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 12/110, Train loss: 0.2544, Train acc: 0.8677, Val loss: 0.4826, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 13/110, Train loss: 0.2231, Train acc: 0.8942, Val loss: 0.5032, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 14/110, Train loss: 0.2212, Train acc: 0.8942, Val loss: 0.5107, Val acc: 0.7531, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 15/110, Train loss: 0.2288, Train acc: 0.8730, Val loss: 0.5151, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 16/110, Train loss: 0.2173, Train acc: 0.8836, Val loss: 0.5259, Val acc: 0.7531, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 17/110, Train loss: 0.2146, Train acc: 0.8836, Val loss: 0.5591, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 18/110, Train loss: 0.2117, Train acc: 0.9206, Val loss: 0.4976, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 19/110, Train loss: 0.2143, Train acc: 0.8889, Val loss: 0.5261, Val acc: 0.7284, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 20/110, Train loss: 0.2216, Train acc: 0.8995, Val loss: 0.5834, Val acc: 0.7531, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 21/110, Train loss: 0.2315, Train acc: 0.8783, Val loss: 0.5430, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 22/110, Train loss: 0.2092, Train acc: 0.9153, Val loss: 0.5691, Val acc: 0.7407, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 23/110, Train loss: 0.2043, Train acc: 0.8995, Val loss: 0.7082, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 24/110, Train loss: 0.2695, Train acc: 0.8730, Val loss: 0.4759, Val acc: 0.7901, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 25/110, Train loss: 0.2118, Train acc: 0.8942, Val loss: 0.5473, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 26/110, Train loss: 0.2039, Train acc: 0.9048, Val loss: 0.5163, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 27/110, Train loss: 0.1999, Train acc: 0.9101, Val loss: 0.5608, Val acc: 0.7160, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 28/110, Train loss: 0.1973, Train acc: 0.8995, Val loss: 0.5800, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 29/110, Train loss: 0.1957, Train acc: 0.8995, Val loss: 0.5812, Val acc: 0.7531, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 30/110, Train loss: 0.1750, Train acc: 0.9153, Val loss: 0.6345, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 31/110, Train loss: 0.2279, Train acc: 0.8624, Val loss: 0.6523, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 32/110, Train loss: 0.2524, Train acc: 0.8624, Val loss: 0.6788, Val acc: 0.8025, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 33/110, Train loss: 0.2463, Train acc: 0.8624, Val loss: 0.7937, Val acc: 0.7407, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 34/110, Train loss: 0.2793, Train acc: 0.8624, Val loss: 0.5240, Val acc: 0.7531, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 35/110, Train loss: 0.2201, Train acc: 0.8942, Val loss: 0.6665, Val acc: 0.7160, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 36/110, Train loss: 0.1868, Train acc: 0.9259, Val loss: 0.6074, Val acc: 0.7531, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 37/110, Train loss: 0.1877, Train acc: 0.9259, Val loss: 0.5680, Val acc: 0.7284, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 38/110, Train loss: 0.1982, Train acc: 0.9101, Val loss: 0.5773, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 39/110, Train loss: 0.2036, Train acc: 0.8995, Val loss: 0.6115, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 40/110, Train loss: 0.2020, Train acc: 0.9101, Val loss: 0.6567, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 41/110, Train loss: 0.1828, Train acc: 0.9153, Val loss: 0.7303, Val acc: 0.7407, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 42/110, Train loss: 0.1672, Train acc: 0.9312, Val loss: 0.6091, Val acc: 0.7531, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 43/110, Train loss: 0.1870, Train acc: 0.9259, Val loss: 0.6735, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 44/110, Train loss: 0.1825, Train acc: 0.9418, Val loss: 0.5946, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 45/110, Train loss: 0.2009, Train acc: 0.9101, Val loss: 0.6848, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 46/110, Train loss: 0.2130, Train acc: 0.8836, Val loss: 0.6856, Val acc: 0.7160, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 47/110, Train loss: 0.2238, Train acc: 0.8730, Val loss: 0.6428, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 48/110, Train loss: 0.2591, Train acc: 0.8783, Val loss: 0.6586, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 49/110, Train loss: 0.1869, Train acc: 0.8942, Val loss: 0.7153, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 50/110, Train loss: 0.2322, Train acc: 0.8677, Val loss: 0.5611, Val acc: 0.7531, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 51/110, Train loss: 0.2325, Train acc: 0.8836, Val loss: 0.6141, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 52/110, Train loss: 0.2153, Train acc: 0.8942, Val loss: 1.0344, Val acc: 0.7654, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 53/110, Train loss: 0.5940, Train acc: 0.7513, Val loss: 0.5921, Val acc: 0.6543, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 54/110, Train loss: 0.4158, Train acc: 0.8360, Val loss: 0.4849, Val acc: 0.8025, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 55/110, Train loss: 0.3785, Train acc: 0.8360, Val loss: 0.4541, Val acc: 0.7778, Best Val loss: 0.4516, Best Val acc: 0.7901\n",
            "Epoch: 56/110, Train loss: 0.3497, Train acc: 0.8360, Val loss: 0.4461, Val acc: 0.8272, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 57/110, Train loss: 0.3206, Train acc: 0.8571, Val loss: 0.4768, Val acc: 0.8025, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 58/110, Train loss: 0.2868, Train acc: 0.8519, Val loss: 0.5744, Val acc: 0.8025, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 59/110, Train loss: 0.3028, Train acc: 0.8519, Val loss: 0.5249, Val acc: 0.8272, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 60/110, Train loss: 0.2604, Train acc: 0.8677, Val loss: 0.5584, Val acc: 0.7901, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 61/110, Train loss: 0.2827, Train acc: 0.8519, Val loss: 0.5798, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 62/110, Train loss: 0.2785, Train acc: 0.8466, Val loss: 0.6285, Val acc: 0.7901, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 63/110, Train loss: 0.2794, Train acc: 0.8624, Val loss: 0.5479, Val acc: 0.7901, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 64/110, Train loss: 0.2395, Train acc: 0.8624, Val loss: 0.5650, Val acc: 0.8148, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 65/110, Train loss: 0.2339, Train acc: 0.8730, Val loss: 0.5860, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 66/110, Train loss: 0.2295, Train acc: 0.8836, Val loss: 0.6573, Val acc: 0.7407, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 67/110, Train loss: 0.2365, Train acc: 0.8677, Val loss: 0.7220, Val acc: 0.7901, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 68/110, Train loss: 0.2554, Train acc: 0.8624, Val loss: 0.4994, Val acc: 0.7901, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 69/110, Train loss: 0.2605, Train acc: 0.8677, Val loss: 0.5320, Val acc: 0.8272, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 70/110, Train loss: 0.2654, Train acc: 0.8677, Val loss: 0.6490, Val acc: 0.7901, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 71/110, Train loss: 0.2370, Train acc: 0.8889, Val loss: 0.5700, Val acc: 0.7654, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 72/110, Train loss: 0.2108, Train acc: 0.9048, Val loss: 0.6014, Val acc: 0.8025, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 73/110, Train loss: 0.2023, Train acc: 0.8995, Val loss: 0.5923, Val acc: 0.7778, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 74/110, Train loss: 0.2121, Train acc: 0.8942, Val loss: 0.6601, Val acc: 0.7654, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 75/110, Train loss: 0.2809, Train acc: 0.8571, Val loss: 0.5708, Val acc: 0.8025, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 76/110, Train loss: 0.2285, Train acc: 0.9101, Val loss: 0.5853, Val acc: 0.7654, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 77/110, Train loss: 0.2210, Train acc: 0.8889, Val loss: 0.6000, Val acc: 0.7778, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 78/110, Train loss: 0.1907, Train acc: 0.9101, Val loss: 0.6215, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 79/110, Train loss: 0.2149, Train acc: 0.8942, Val loss: 0.6484, Val acc: 0.7901, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 80/110, Train loss: 0.2299, Train acc: 0.8836, Val loss: 0.6165, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 81/110, Train loss: 0.2015, Train acc: 0.9101, Val loss: 0.7102, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 82/110, Train loss: 0.2102, Train acc: 0.8783, Val loss: 0.6328, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 83/110, Train loss: 0.2054, Train acc: 0.8889, Val loss: 0.6832, Val acc: 0.7407, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 84/110, Train loss: 0.2724, Train acc: 0.8889, Val loss: 0.6284, Val acc: 0.7284, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 85/110, Train loss: 0.2476, Train acc: 0.8624, Val loss: 0.6789, Val acc: 0.8148, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 86/110, Train loss: 0.2760, Train acc: 0.8677, Val loss: 0.5925, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 87/110, Train loss: 0.1950, Train acc: 0.9153, Val loss: 0.6452, Val acc: 0.7778, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 88/110, Train loss: 0.2001, Train acc: 0.9101, Val loss: 0.6449, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 89/110, Train loss: 0.2251, Train acc: 0.8783, Val loss: 0.6455, Val acc: 0.7901, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 90/110, Train loss: 0.2062, Train acc: 0.8889, Val loss: 0.6548, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 91/110, Train loss: 0.2272, Train acc: 0.8942, Val loss: 0.5592, Val acc: 0.7778, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 92/110, Train loss: 0.1928, Train acc: 0.9153, Val loss: 0.6221, Val acc: 0.7407, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 93/110, Train loss: 0.1748, Train acc: 0.9206, Val loss: 0.7171, Val acc: 0.7407, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 94/110, Train loss: 0.2010, Train acc: 0.8942, Val loss: 0.6055, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 95/110, Train loss: 0.1856, Train acc: 0.9153, Val loss: 0.5518, Val acc: 0.7778, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 96/110, Train loss: 0.1452, Train acc: 0.9471, Val loss: 0.6903, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 97/110, Train loss: 0.1504, Train acc: 0.9524, Val loss: 0.6275, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 98/110, Train loss: 0.1539, Train acc: 0.9312, Val loss: 0.6163, Val acc: 0.7654, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 99/110, Train loss: 0.1318, Train acc: 0.9471, Val loss: 0.7406, Val acc: 0.7407, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 100/110, Train loss: 0.1394, Train acc: 0.9524, Val loss: 0.7033, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 101/110, Train loss: 0.1352, Train acc: 0.9418, Val loss: 0.6777, Val acc: 0.7778, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 102/110, Train loss: 0.1399, Train acc: 0.9418, Val loss: 0.7311, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 103/110, Train loss: 0.1315, Train acc: 0.9524, Val loss: 0.6535, Val acc: 0.8148, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 104/110, Train loss: 0.1356, Train acc: 0.9418, Val loss: 0.7165, Val acc: 0.7531, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 105/110, Train loss: 0.1868, Train acc: 0.8995, Val loss: 0.7211, Val acc: 0.8025, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 106/110, Train loss: 0.2097, Train acc: 0.9101, Val loss: 0.7215, Val acc: 0.7778, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 107/110, Train loss: 0.2028, Train acc: 0.9206, Val loss: 0.6456, Val acc: 0.7778, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 108/110, Train loss: 0.1657, Train acc: 0.9418, Val loss: 0.6030, Val acc: 0.7901, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 109/110, Train loss: 0.1481, Train acc: 0.9418, Val loss: 0.7286, Val acc: 0.7778, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Epoch: 110/110, Train loss: 0.1624, Train acc: 0.9312, Val loss: 0.7529, Val acc: 0.7778, Best Val loss: 0.4461, Best Val acc: 0.8272\n",
            "Training with batch size: 32, epochs: 90\n",
            "Epoch: 1/90, Train loss: 0.1687, Train acc: 0.9206, Val loss: 0.7845, Val acc: 0.7531, Best Val loss: 0.7845, Best Val acc: 0.7531\n",
            "Epoch: 2/90, Train loss: 0.1823, Train acc: 0.9153, Val loss: 0.7664, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 3/90, Train loss: 0.1332, Train acc: 0.9312, Val loss: 0.8196, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 4/90, Train loss: 0.1174, Train acc: 0.9365, Val loss: 0.9277, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 5/90, Train loss: 0.1190, Train acc: 0.9524, Val loss: 0.9681, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 6/90, Train loss: 0.0982, Train acc: 0.9683, Val loss: 0.9672, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 7/90, Train loss: 0.0992, Train acc: 0.9577, Val loss: 0.9044, Val acc: 0.7778, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 8/90, Train loss: 0.0870, Train acc: 0.9735, Val loss: 0.9335, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 9/90, Train loss: 0.0875, Train acc: 0.9735, Val loss: 0.9514, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 10/90, Train loss: 0.0843, Train acc: 0.9735, Val loss: 0.9551, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 11/90, Train loss: 0.0788, Train acc: 0.9788, Val loss: 1.0082, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 12/90, Train loss: 0.0796, Train acc: 0.9788, Val loss: 1.0494, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 13/90, Train loss: 0.0785, Train acc: 0.9788, Val loss: 1.0208, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 14/90, Train loss: 0.0736, Train acc: 0.9735, Val loss: 1.0095, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 15/90, Train loss: 0.0740, Train acc: 0.9788, Val loss: 1.0273, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 16/90, Train loss: 0.0708, Train acc: 0.9841, Val loss: 1.0270, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 17/90, Train loss: 0.0694, Train acc: 0.9788, Val loss: 1.0823, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 18/90, Train loss: 0.0831, Train acc: 0.9788, Val loss: 1.1322, Val acc: 0.7778, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 19/90, Train loss: 0.0780, Train acc: 0.9788, Val loss: 1.1220, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 20/90, Train loss: 0.0756, Train acc: 0.9735, Val loss: 1.0935, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 21/90, Train loss: 0.0678, Train acc: 0.9841, Val loss: 1.1347, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 22/90, Train loss: 0.0673, Train acc: 0.9735, Val loss: 1.0586, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 23/90, Train loss: 0.0627, Train acc: 0.9788, Val loss: 1.1271, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 24/90, Train loss: 0.0617, Train acc: 0.9735, Val loss: 1.2311, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 25/90, Train loss: 0.0619, Train acc: 0.9788, Val loss: 1.2498, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 26/90, Train loss: 0.0650, Train acc: 0.9788, Val loss: 1.1827, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 27/90, Train loss: 0.0737, Train acc: 0.9735, Val loss: 1.1736, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 28/90, Train loss: 0.1063, Train acc: 0.9683, Val loss: 1.0637, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 29/90, Train loss: 0.0911, Train acc: 0.9683, Val loss: 1.0391, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 30/90, Train loss: 0.0897, Train acc: 0.9683, Val loss: 1.0825, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 31/90, Train loss: 0.0627, Train acc: 0.9894, Val loss: 1.1623, Val acc: 0.7778, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 32/90, Train loss: 0.0716, Train acc: 0.9683, Val loss: 1.1386, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 33/90, Train loss: 0.0574, Train acc: 0.9894, Val loss: 1.0705, Val acc: 0.7407, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 34/90, Train loss: 0.0533, Train acc: 0.9947, Val loss: 1.0978, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 35/90, Train loss: 0.0482, Train acc: 0.9947, Val loss: 1.2019, Val acc: 0.7407, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 36/90, Train loss: 0.0452, Train acc: 0.9894, Val loss: 1.2208, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 37/90, Train loss: 0.0470, Train acc: 0.9894, Val loss: 1.1781, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 38/90, Train loss: 0.0455, Train acc: 0.9947, Val loss: 1.2267, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 39/90, Train loss: 0.0506, Train acc: 0.9894, Val loss: 1.2312, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 40/90, Train loss: 0.0491, Train acc: 0.9894, Val loss: 1.2309, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 41/90, Train loss: 0.0455, Train acc: 0.9894, Val loss: 1.1920, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 42/90, Train loss: 0.0409, Train acc: 0.9947, Val loss: 1.2400, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 43/90, Train loss: 0.0414, Train acc: 0.9947, Val loss: 1.3105, Val acc: 0.7778, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 44/90, Train loss: 0.0445, Train acc: 0.9947, Val loss: 1.3290, Val acc: 0.7407, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 45/90, Train loss: 0.0359, Train acc: 0.9894, Val loss: 1.2704, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 46/90, Train loss: 0.0399, Train acc: 0.9894, Val loss: 1.2999, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 47/90, Train loss: 0.0379, Train acc: 0.9947, Val loss: 1.3349, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 48/90, Train loss: 0.0362, Train acc: 0.9947, Val loss: 1.3842, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 49/90, Train loss: 0.0366, Train acc: 0.9947, Val loss: 1.3494, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 50/90, Train loss: 0.0411, Train acc: 0.9947, Val loss: 1.3165, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 51/90, Train loss: 0.0394, Train acc: 0.9947, Val loss: 1.3486, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 52/90, Train loss: 0.0403, Train acc: 0.9894, Val loss: 1.4457, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 53/90, Train loss: 0.0406, Train acc: 0.9947, Val loss: 1.3683, Val acc: 0.7407, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 54/90, Train loss: 0.0398, Train acc: 0.9947, Val loss: 1.2616, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 55/90, Train loss: 0.0337, Train acc: 1.0000, Val loss: 1.3751, Val acc: 0.7407, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 56/90, Train loss: 0.0358, Train acc: 0.9947, Val loss: 1.4834, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 57/90, Train loss: 0.0641, Train acc: 0.9788, Val loss: 1.3116, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 58/90, Train loss: 0.1008, Train acc: 0.9683, Val loss: 1.2639, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 59/90, Train loss: 0.0913, Train acc: 0.9577, Val loss: 1.4239, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 60/90, Train loss: 0.1075, Train acc: 0.9524, Val loss: 1.1994, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 61/90, Train loss: 0.0726, Train acc: 0.9735, Val loss: 1.2255, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 62/90, Train loss: 0.1144, Train acc: 0.9471, Val loss: 1.3056, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 63/90, Train loss: 0.2535, Train acc: 0.8889, Val loss: 1.3795, Val acc: 0.7778, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 64/90, Train loss: 0.5842, Train acc: 0.8571, Val loss: 1.0510, Val acc: 0.7284, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 65/90, Train loss: 0.3621, Train acc: 0.8413, Val loss: 0.8763, Val acc: 0.7531, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 66/90, Train loss: 0.2086, Train acc: 0.9206, Val loss: 0.8032, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 67/90, Train loss: 0.1941, Train acc: 0.8889, Val loss: 0.8885, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 68/90, Train loss: 0.1508, Train acc: 0.9206, Val loss: 0.8002, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 69/90, Train loss: 0.1455, Train acc: 0.9153, Val loss: 0.8180, Val acc: 0.8025, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 70/90, Train loss: 0.1481, Train acc: 0.9312, Val loss: 0.8543, Val acc: 0.8025, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 71/90, Train loss: 0.1592, Train acc: 0.9101, Val loss: 0.8429, Val acc: 0.8025, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 72/90, Train loss: 0.1490, Train acc: 0.9312, Val loss: 0.8480, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 73/90, Train loss: 0.1318, Train acc: 0.9418, Val loss: 0.9443, Val acc: 0.8148, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 74/90, Train loss: 0.1346, Train acc: 0.9259, Val loss: 0.9006, Val acc: 0.7654, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 75/90, Train loss: 0.1198, Train acc: 0.9418, Val loss: 0.8407, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 76/90, Train loss: 0.1053, Train acc: 0.9524, Val loss: 0.8922, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 77/90, Train loss: 0.0947, Train acc: 0.9577, Val loss: 0.9140, Val acc: 0.8025, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 78/90, Train loss: 0.0795, Train acc: 0.9683, Val loss: 0.8745, Val acc: 0.8025, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 79/90, Train loss: 0.0614, Train acc: 0.9894, Val loss: 0.9218, Val acc: 0.8025, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 80/90, Train loss: 0.0622, Train acc: 0.9894, Val loss: 1.0206, Val acc: 0.8025, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 81/90, Train loss: 0.0518, Train acc: 1.0000, Val loss: 0.9831, Val acc: 0.7778, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 82/90, Train loss: 0.0480, Train acc: 1.0000, Val loss: 1.0047, Val acc: 0.7778, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 83/90, Train loss: 0.0447, Train acc: 0.9947, Val loss: 1.0464, Val acc: 0.7778, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 84/90, Train loss: 0.0402, Train acc: 0.9947, Val loss: 1.0318, Val acc: 0.7778, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 85/90, Train loss: 0.0420, Train acc: 0.9947, Val loss: 1.0507, Val acc: 0.7901, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 86/90, Train loss: 0.0460, Train acc: 0.9894, Val loss: 1.0378, Val acc: 0.8272, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 87/90, Train loss: 0.0539, Train acc: 0.9841, Val loss: 1.0299, Val acc: 0.7778, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 88/90, Train loss: 0.0491, Train acc: 0.9947, Val loss: 1.0430, Val acc: 0.8148, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 89/90, Train loss: 0.0399, Train acc: 0.9947, Val loss: 1.0744, Val acc: 0.8272, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Epoch: 90/90, Train loss: 0.0432, Train acc: 0.9947, Val loss: 1.0387, Val acc: 0.8148, Best Val loss: 0.7664, Best Val acc: 0.7654\n",
            "Training with batch size: 32, epochs: 100\n",
            "Epoch: 1/100, Train loss: 0.0465, Train acc: 0.9894, Val loss: 1.1080, Val acc: 0.7901, Best Val loss: 1.1080, Best Val acc: 0.7901\n",
            "Epoch: 2/100, Train loss: 0.0399, Train acc: 0.9894, Val loss: 1.1088, Val acc: 0.7654, Best Val loss: 1.1080, Best Val acc: 0.7901\n",
            "Epoch: 3/100, Train loss: 0.0357, Train acc: 0.9947, Val loss: 1.1205, Val acc: 0.7778, Best Val loss: 1.1080, Best Val acc: 0.7901\n",
            "Epoch: 4/100, Train loss: 0.0291, Train acc: 1.0000, Val loss: 1.0631, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 5/100, Train loss: 0.0345, Train acc: 0.9947, Val loss: 1.0842, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 6/100, Train loss: 0.0260, Train acc: 1.0000, Val loss: 1.1457, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 7/100, Train loss: 0.0268, Train acc: 1.0000, Val loss: 1.1608, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 8/100, Train loss: 0.0204, Train acc: 1.0000, Val loss: 1.1601, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 9/100, Train loss: 0.0215, Train acc: 1.0000, Val loss: 1.1533, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 10/100, Train loss: 0.0192, Train acc: 1.0000, Val loss: 1.1809, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 11/100, Train loss: 0.0176, Train acc: 1.0000, Val loss: 1.1868, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 12/100, Train loss: 0.0171, Train acc: 1.0000, Val loss: 1.1922, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 13/100, Train loss: 0.0163, Train acc: 1.0000, Val loss: 1.1955, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 14/100, Train loss: 0.0169, Train acc: 1.0000, Val loss: 1.2197, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 15/100, Train loss: 0.0158, Train acc: 1.0000, Val loss: 1.2063, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 16/100, Train loss: 0.0154, Train acc: 1.0000, Val loss: 1.2319, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 17/100, Train loss: 0.0151, Train acc: 1.0000, Val loss: 1.2434, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 18/100, Train loss: 0.0162, Train acc: 1.0000, Val loss: 1.2424, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 19/100, Train loss: 0.0147, Train acc: 1.0000, Val loss: 1.2692, Val acc: 0.8025, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 20/100, Train loss: 0.0148, Train acc: 1.0000, Val loss: 1.2498, Val acc: 0.7654, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 21/100, Train loss: 0.0130, Train acc: 1.0000, Val loss: 1.2683, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 22/100, Train loss: 0.0136, Train acc: 1.0000, Val loss: 1.2756, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 23/100, Train loss: 0.0136, Train acc: 1.0000, Val loss: 1.2883, Val acc: 0.8025, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 24/100, Train loss: 0.0136, Train acc: 1.0000, Val loss: 1.3003, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 25/100, Train loss: 0.0114, Train acc: 1.0000, Val loss: 1.3147, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 26/100, Train loss: 0.0111, Train acc: 1.0000, Val loss: 1.3059, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 27/100, Train loss: 0.0121, Train acc: 1.0000, Val loss: 1.2834, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 28/100, Train loss: 0.0110, Train acc: 1.0000, Val loss: 1.3232, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 29/100, Train loss: 0.0120, Train acc: 1.0000, Val loss: 1.3511, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 30/100, Train loss: 0.0102, Train acc: 1.0000, Val loss: 1.3486, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 31/100, Train loss: 0.0109, Train acc: 1.0000, Val loss: 1.3459, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 32/100, Train loss: 0.0111, Train acc: 1.0000, Val loss: 1.3387, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 33/100, Train loss: 0.0098, Train acc: 1.0000, Val loss: 1.3623, Val acc: 0.7654, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 34/100, Train loss: 0.0099, Train acc: 1.0000, Val loss: 1.3814, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 35/100, Train loss: 0.0091, Train acc: 1.0000, Val loss: 1.3633, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 36/100, Train loss: 0.0088, Train acc: 1.0000, Val loss: 1.3909, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 37/100, Train loss: 0.0084, Train acc: 1.0000, Val loss: 1.3920, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 38/100, Train loss: 0.0088, Train acc: 1.0000, Val loss: 1.4046, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 39/100, Train loss: 0.0085, Train acc: 1.0000, Val loss: 1.3988, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 40/100, Train loss: 0.0089, Train acc: 1.0000, Val loss: 1.4070, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 41/100, Train loss: 0.0093, Train acc: 1.0000, Val loss: 1.4024, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 42/100, Train loss: 0.0086, Train acc: 1.0000, Val loss: 1.4127, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 43/100, Train loss: 0.0078, Train acc: 1.0000, Val loss: 1.4337, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 44/100, Train loss: 0.0075, Train acc: 1.0000, Val loss: 1.4177, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 45/100, Train loss: 0.0070, Train acc: 1.0000, Val loss: 1.4417, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 46/100, Train loss: 0.0072, Train acc: 1.0000, Val loss: 1.4386, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 47/100, Train loss: 0.0067, Train acc: 1.0000, Val loss: 1.4594, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 48/100, Train loss: 0.0068, Train acc: 1.0000, Val loss: 1.4577, Val acc: 0.7654, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 49/100, Train loss: 0.0065, Train acc: 1.0000, Val loss: 1.4723, Val acc: 0.7654, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 50/100, Train loss: 0.0063, Train acc: 1.0000, Val loss: 1.4842, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 51/100, Train loss: 0.0062, Train acc: 1.0000, Val loss: 1.4780, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 52/100, Train loss: 0.0060, Train acc: 1.0000, Val loss: 1.4838, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 53/100, Train loss: 0.0059, Train acc: 1.0000, Val loss: 1.5009, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 54/100, Train loss: 0.0062, Train acc: 1.0000, Val loss: 1.5191, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 55/100, Train loss: 0.0062, Train acc: 1.0000, Val loss: 1.4742, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 56/100, Train loss: 0.0064, Train acc: 1.0000, Val loss: 1.5024, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 57/100, Train loss: 0.0056, Train acc: 1.0000, Val loss: 1.5331, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 58/100, Train loss: 0.0053, Train acc: 1.0000, Val loss: 1.5331, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 59/100, Train loss: 0.0054, Train acc: 1.0000, Val loss: 1.5382, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 60/100, Train loss: 0.0051, Train acc: 1.0000, Val loss: 1.5320, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 61/100, Train loss: 0.0050, Train acc: 1.0000, Val loss: 1.5397, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 62/100, Train loss: 0.0048, Train acc: 1.0000, Val loss: 1.5620, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 63/100, Train loss: 0.0047, Train acc: 1.0000, Val loss: 1.5672, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 64/100, Train loss: 0.0046, Train acc: 1.0000, Val loss: 1.5628, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 65/100, Train loss: 0.0048, Train acc: 1.0000, Val loss: 1.5613, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 66/100, Train loss: 0.0046, Train acc: 1.0000, Val loss: 1.5670, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 67/100, Train loss: 0.0046, Train acc: 1.0000, Val loss: 1.5855, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 68/100, Train loss: 0.0047, Train acc: 1.0000, Val loss: 1.5953, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 69/100, Train loss: 0.0044, Train acc: 1.0000, Val loss: 1.5862, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 70/100, Train loss: 0.0043, Train acc: 1.0000, Val loss: 1.5724, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 71/100, Train loss: 0.0042, Train acc: 1.0000, Val loss: 1.6036, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 72/100, Train loss: 0.0042, Train acc: 1.0000, Val loss: 1.6179, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 73/100, Train loss: 0.0041, Train acc: 1.0000, Val loss: 1.6101, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 74/100, Train loss: 0.0038, Train acc: 1.0000, Val loss: 1.6156, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 75/100, Train loss: 0.0038, Train acc: 1.0000, Val loss: 1.6267, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 76/100, Train loss: 0.0037, Train acc: 1.0000, Val loss: 1.6323, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 77/100, Train loss: 0.0036, Train acc: 1.0000, Val loss: 1.6395, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 78/100, Train loss: 0.0037, Train acc: 1.0000, Val loss: 1.6455, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 79/100, Train loss: 0.0037, Train acc: 1.0000, Val loss: 1.6318, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 80/100, Train loss: 0.0035, Train acc: 1.0000, Val loss: 1.6587, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 81/100, Train loss: 0.0034, Train acc: 1.0000, Val loss: 1.6644, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 82/100, Train loss: 0.0035, Train acc: 1.0000, Val loss: 1.6796, Val acc: 0.7654, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 83/100, Train loss: 0.0033, Train acc: 1.0000, Val loss: 1.6604, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 84/100, Train loss: 0.0033, Train acc: 1.0000, Val loss: 1.6659, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 85/100, Train loss: 0.0032, Train acc: 1.0000, Val loss: 1.6886, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 86/100, Train loss: 0.0032, Train acc: 1.0000, Val loss: 1.6907, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 87/100, Train loss: 0.0031, Train acc: 1.0000, Val loss: 1.6921, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 88/100, Train loss: 0.0032, Train acc: 1.0000, Val loss: 1.6995, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 89/100, Train loss: 0.0031, Train acc: 1.0000, Val loss: 1.7098, Val acc: 0.7654, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 90/100, Train loss: 0.0031, Train acc: 1.0000, Val loss: 1.7181, Val acc: 0.7531, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 91/100, Train loss: 0.0029, Train acc: 1.0000, Val loss: 1.7124, Val acc: 0.7654, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 92/100, Train loss: 0.0029, Train acc: 1.0000, Val loss: 1.7249, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 93/100, Train loss: 0.0030, Train acc: 1.0000, Val loss: 1.7162, Val acc: 0.7654, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 94/100, Train loss: 0.0029, Train acc: 1.0000, Val loss: 1.7376, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 95/100, Train loss: 0.0027, Train acc: 1.0000, Val loss: 1.7511, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 96/100, Train loss: 0.0027, Train acc: 1.0000, Val loss: 1.7409, Val acc: 0.7531, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 97/100, Train loss: 0.0027, Train acc: 1.0000, Val loss: 1.7526, Val acc: 0.7531, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 98/100, Train loss: 0.0026, Train acc: 1.0000, Val loss: 1.7527, Val acc: 0.7531, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 99/100, Train loss: 0.0026, Train acc: 1.0000, Val loss: 1.7597, Val acc: 0.7778, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Epoch: 100/100, Train loss: 0.0026, Train acc: 1.0000, Val loss: 1.7680, Val acc: 0.7901, Best Val loss: 1.0631, Best Val acc: 0.7901\n",
            "Training with batch size: 32, epochs: 110\n",
            "Epoch: 1/110, Train loss: 0.0027, Train acc: 1.0000, Val loss: 1.7736, Val acc: 0.7654, Best Val loss: 1.7736, Best Val acc: 0.7654\n",
            "Epoch: 2/110, Train loss: 0.0024, Train acc: 1.0000, Val loss: 1.7719, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 3/110, Train loss: 0.0024, Train acc: 1.0000, Val loss: 1.7775, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 4/110, Train loss: 0.0025, Train acc: 1.0000, Val loss: 1.7877, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 5/110, Train loss: 0.0024, Train acc: 1.0000, Val loss: 1.7804, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 6/110, Train loss: 0.0024, Train acc: 1.0000, Val loss: 1.7984, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 7/110, Train loss: 0.0023, Train acc: 1.0000, Val loss: 1.7891, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 8/110, Train loss: 0.0023, Train acc: 1.0000, Val loss: 1.8070, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 9/110, Train loss: 0.0023, Train acc: 1.0000, Val loss: 1.8170, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 10/110, Train loss: 0.0023, Train acc: 1.0000, Val loss: 1.8043, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 11/110, Train loss: 0.0022, Train acc: 1.0000, Val loss: 1.8250, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 12/110, Train loss: 0.0022, Train acc: 1.0000, Val loss: 1.8276, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 13/110, Train loss: 0.0021, Train acc: 1.0000, Val loss: 1.8324, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 14/110, Train loss: 0.0021, Train acc: 1.0000, Val loss: 1.8337, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 15/110, Train loss: 0.0020, Train acc: 1.0000, Val loss: 1.8370, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 16/110, Train loss: 0.0020, Train acc: 1.0000, Val loss: 1.8516, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 17/110, Train loss: 0.0019, Train acc: 1.0000, Val loss: 1.8580, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 18/110, Train loss: 0.0019, Train acc: 1.0000, Val loss: 1.8531, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 19/110, Train loss: 0.0020, Train acc: 1.0000, Val loss: 1.8486, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 20/110, Train loss: 0.0020, Train acc: 1.0000, Val loss: 1.8652, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 21/110, Train loss: 0.0019, Train acc: 1.0000, Val loss: 1.8716, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 22/110, Train loss: 0.0019, Train acc: 1.0000, Val loss: 1.8776, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 23/110, Train loss: 0.0019, Train acc: 1.0000, Val loss: 1.8682, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 24/110, Train loss: 0.0019, Train acc: 1.0000, Val loss: 1.8714, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 25/110, Train loss: 0.0019, Train acc: 1.0000, Val loss: 1.8827, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 26/110, Train loss: 0.0018, Train acc: 1.0000, Val loss: 1.8837, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 27/110, Train loss: 0.0018, Train acc: 1.0000, Val loss: 1.8821, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 28/110, Train loss: 0.0017, Train acc: 1.0000, Val loss: 1.8962, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 29/110, Train loss: 0.0017, Train acc: 1.0000, Val loss: 1.9053, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 30/110, Train loss: 0.0017, Train acc: 1.0000, Val loss: 1.9099, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 31/110, Train loss: 0.0017, Train acc: 1.0000, Val loss: 1.9186, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 32/110, Train loss: 0.0017, Train acc: 1.0000, Val loss: 1.9081, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 33/110, Train loss: 0.0016, Train acc: 1.0000, Val loss: 1.9134, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 34/110, Train loss: 0.0016, Train acc: 1.0000, Val loss: 1.9259, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 35/110, Train loss: 0.0017, Train acc: 1.0000, Val loss: 1.9369, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 36/110, Train loss: 0.0016, Train acc: 1.0000, Val loss: 1.9433, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 37/110, Train loss: 0.0017, Train acc: 1.0000, Val loss: 1.9339, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 38/110, Train loss: 0.0016, Train acc: 1.0000, Val loss: 1.9413, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 39/110, Train loss: 0.0015, Train acc: 1.0000, Val loss: 1.9437, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 40/110, Train loss: 0.0015, Train acc: 1.0000, Val loss: 1.9392, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 41/110, Train loss: 0.0015, Train acc: 1.0000, Val loss: 1.9530, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 42/110, Train loss: 0.0015, Train acc: 1.0000, Val loss: 1.9641, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 43/110, Train loss: 0.0015, Train acc: 1.0000, Val loss: 1.9613, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 44/110, Train loss: 0.0014, Train acc: 1.0000, Val loss: 1.9683, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 45/110, Train loss: 0.0015, Train acc: 1.0000, Val loss: 1.9671, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 46/110, Train loss: 0.0014, Train acc: 1.0000, Val loss: 1.9710, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 47/110, Train loss: 0.0015, Train acc: 1.0000, Val loss: 1.9823, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 48/110, Train loss: 0.0014, Train acc: 1.0000, Val loss: 1.9858, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 49/110, Train loss: 0.0014, Train acc: 1.0000, Val loss: 1.9771, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 50/110, Train loss: 0.0014, Train acc: 1.0000, Val loss: 1.9753, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 51/110, Train loss: 0.0014, Train acc: 1.0000, Val loss: 1.9774, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 52/110, Train loss: 0.0013, Train acc: 1.0000, Val loss: 1.9959, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 53/110, Train loss: 0.0014, Train acc: 1.0000, Val loss: 2.0044, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 54/110, Train loss: 0.0013, Train acc: 1.0000, Val loss: 1.9988, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 55/110, Train loss: 0.0013, Train acc: 1.0000, Val loss: 1.9992, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 56/110, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.0047, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 57/110, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.0166, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 58/110, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.0191, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 59/110, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.0186, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 60/110, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.0154, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 61/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.0290, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 62/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.0325, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 63/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.0330, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 64/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.0387, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 65/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.0431, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 66/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.0472, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 67/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.0504, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 68/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.0545, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 69/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.0534, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 70/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.0520, Val acc: 0.7778, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 71/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.0586, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 72/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.0664, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 73/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.0710, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 74/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.0810, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 75/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.0668, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 76/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.0643, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 77/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.0775, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 78/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.0893, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 79/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.0931, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 80/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.0923, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 81/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.0882, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 82/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.0894, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 83/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.0961, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 84/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.1056, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 85/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.1070, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 86/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.1122, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 87/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.1175, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 88/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.1158, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 89/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.1209, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 90/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.1291, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 91/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.1366, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 92/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.1333, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 93/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.1249, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 94/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.1298, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 95/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.1350, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 96/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.1504, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 97/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.1495, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 98/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.1514, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 99/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.1541, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 100/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.1595, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 101/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.1576, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 102/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.1589, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 103/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.1680, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 104/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.1699, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 105/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.1680, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 106/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.1712, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 107/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.1758, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 108/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.1860, Val acc: 0.7654, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 109/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.1840, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Epoch: 110/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.1853, Val acc: 0.7531, Best Val loss: 1.7719, Best Val acc: 0.7531\n",
            "Training with batch size: 64, epochs: 90\n",
            "Epoch: 1/90, Train loss: 0.0008, Train acc: 1.0000, Val loss: 1.9734, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 2/90, Train loss: 0.0008, Train acc: 1.0000, Val loss: 1.9742, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 3/90, Train loss: 0.0008, Train acc: 1.0000, Val loss: 1.9774, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 4/90, Train loss: 0.0008, Train acc: 1.0000, Val loss: 1.9786, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 5/90, Train loss: 0.0008, Train acc: 1.0000, Val loss: 1.9802, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 6/90, Train loss: 0.0008, Train acc: 1.0000, Val loss: 1.9848, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 7/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9885, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 8/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9900, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 9/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9906, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 10/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9896, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 11/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9900, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 12/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9937, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 13/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9959, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 14/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9950, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 15/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9948, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 16/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9965, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 17/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9975, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 18/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 1.9981, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 19/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0011, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 20/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0025, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 21/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0025, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 22/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0042, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 23/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0057, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 24/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0055, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 25/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0072, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 26/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0083, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 27/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0104, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 28/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0128, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 29/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0133, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 30/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0131, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 31/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0153, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 32/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0185, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 33/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0188, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 34/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0168, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 35/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0194, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 36/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0209, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 37/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0212, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 38/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0199, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 39/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0231, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 40/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0281, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 41/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0303, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 42/90, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.0316, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 43/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0324, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 44/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0307, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 45/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0295, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 46/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0325, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 47/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0358, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 48/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0386, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 49/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0396, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 50/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0386, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 51/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0363, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 52/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0371, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 53/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0389, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 54/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0421, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 55/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0450, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 56/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0444, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 57/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0444, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 58/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0458, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 59/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0490, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 60/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0508, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 61/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0501, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 62/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0514, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 63/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0524, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 64/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0514, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 65/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0535, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 66/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0569, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 67/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0591, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 68/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0602, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 69/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0586, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 70/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0582, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 71/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0604, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 72/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0622, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 73/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0628, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 74/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0647, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 75/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0649, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 76/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0662, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 77/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0672, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 78/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0670, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 79/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0693, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 80/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0727, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 81/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0732, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 82/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0729, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 83/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0747, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 84/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0762, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 85/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0776, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 86/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0776, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 87/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0771, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 88/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0794, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 89/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0813, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Epoch: 90/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0848, Val acc: 0.7531, Best Val loss: 1.9734, Best Val acc: 0.7531\n",
            "Training with batch size: 64, epochs: 100\n",
            "Epoch: 1/100, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0854, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 2/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0855, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 3/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0869, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 4/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0874, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 5/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0884, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 6/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0886, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 7/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0919, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 8/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0920, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 9/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0914, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 10/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0908, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 11/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0941, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 12/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0949, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 13/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0955, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 14/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0970, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 15/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0971, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 16/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0956, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 17/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0967, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 18/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0986, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 19/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1035, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 20/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1061, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 21/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1067, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 22/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1052, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 23/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1046, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 24/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1061, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 25/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1088, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 26/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1105, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 27/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1133, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 28/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1119, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 29/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1111, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 30/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1125, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 31/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1148, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 32/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1158, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 33/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1180, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 34/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1174, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 35/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1190, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 36/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1186, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 37/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1201, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 38/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1220, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 39/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1232, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 40/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1251, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 41/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1264, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 42/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1258, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 43/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1259, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 44/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1280, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 45/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1287, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 46/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1297, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 47/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1319, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 48/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1335, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 49/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1318, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 50/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1323, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 51/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1333, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 52/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1373, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 53/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1365, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 54/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1376, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 55/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1381, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 56/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1396, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 57/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1405, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 58/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1422, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 59/100, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1416, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 60/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1436, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 61/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1434, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 62/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1438, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 63/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1443, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 64/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1459, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 65/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1469, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 66/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1507, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 67/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1524, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 68/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1529, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 69/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1540, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 70/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1548, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 71/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1555, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 72/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1566, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 73/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1593, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 74/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1609, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 75/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1601, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 76/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1603, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 77/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1602, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 78/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1608, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 79/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1641, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 80/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1666, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 81/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1677, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 82/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1682, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 83/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1671, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 84/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1690, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 85/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1699, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 86/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1715, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 87/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1735, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 88/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1747, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 89/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1739, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 90/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1747, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 91/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1752, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 92/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1766, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 93/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1766, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 94/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1779, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 95/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1808, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 96/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1825, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 97/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1852, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 98/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1842, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 99/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1822, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Epoch: 100/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1839, Val acc: 0.7531, Best Val loss: 2.0854, Best Val acc: 0.7531\n",
            "Training with batch size: 64, epochs: 110\n",
            "Epoch: 1/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1853, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 2/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1876, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 3/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1898, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 4/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1903, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 5/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1888, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 6/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1886, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 7/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1898, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 8/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1918, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 9/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1947, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 10/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1968, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 11/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1971, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 12/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1971, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 13/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1970, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 14/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1974, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 15/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1981, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 16/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1986, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 17/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2003, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 18/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1999, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 19/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2042, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 20/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2077, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 21/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2084, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 22/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2065, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 23/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2057, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 24/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2053, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 25/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2073, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 26/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2082, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 27/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2106, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 28/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2118, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 29/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2115, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 30/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2127, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 31/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2134, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 32/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2137, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 33/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2151, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 34/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2151, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 35/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2159, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 36/110, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.2182, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 37/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2185, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 38/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2189, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 39/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2211, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 40/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2212, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 41/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2208, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 42/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2214, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 43/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2221, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 44/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2244, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 45/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2261, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 46/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2269, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 47/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2272, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 48/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2259, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 49/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2266, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 50/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2299, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 51/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2311, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 52/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2322, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 53/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2328, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 54/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2332, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 55/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2330, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 56/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2334, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 57/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2362, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 58/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2372, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 59/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2372, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 60/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2377, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 61/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2370, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 62/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2369, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 63/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2387, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 64/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2419, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 65/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2445, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 66/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2433, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 67/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2417, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 68/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2418, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 69/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2434, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 70/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2450, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 71/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2469, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 72/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2475, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 73/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2474, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 74/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2490, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 75/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2503, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 76/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2508, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 77/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2506, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 78/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2515, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 79/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2539, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 80/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2549, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 81/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2556, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 82/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2562, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 83/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2552, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 84/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2553, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 85/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2550, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 86/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2550, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 87/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2579, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 88/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2607, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 89/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2612, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 90/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2617, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 91/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2620, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 92/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2637, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 93/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2661, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 94/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2654, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 95/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2650, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 96/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2653, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 97/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2680, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 98/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2684, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 99/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2700, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 100/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2718, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 101/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2719, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 102/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2718, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 103/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2713, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 104/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2719, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 105/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2722, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 106/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2745, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 107/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2762, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 108/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2775, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 109/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2784, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Epoch: 110/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2788, Val acc: 0.7531, Best Val loss: 2.1853, Best Val acc: 0.7531\n",
            "Training with batch size: 16, epochs: 90\n",
            "Epoch: 1/90, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2843, Val acc: 0.7531, Best Val loss: 2.2843, Best Val acc: 0.7531\n",
            "Epoch: 2/90, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2971, Val acc: 0.7531, Best Val loss: 2.2843, Best Val acc: 0.7531\n",
            "Epoch: 3/90, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2804, Val acc: 0.7654, Best Val loss: 2.2804, Best Val acc: 0.7654\n",
            "Epoch: 4/90, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.3056, Val acc: 0.7531, Best Val loss: 2.2804, Best Val acc: 0.7654\n",
            "Epoch: 5/90, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.3065, Val acc: 0.7531, Best Val loss: 2.2804, Best Val acc: 0.7654\n",
            "Epoch: 6/90, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2935, Val acc: 0.7531, Best Val loss: 2.2804, Best Val acc: 0.7654\n",
            "Epoch: 7/90, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.3156, Val acc: 0.7407, Best Val loss: 2.2804, Best Val acc: 0.7654\n",
            "Epoch: 8/90, Train loss: 0.0016, Train acc: 1.0000, Val loss: 2.6161, Val acc: 0.7531, Best Val loss: 2.2804, Best Val acc: 0.7654\n",
            "Epoch: 9/90, Train loss: 2.7144, Train acc: 0.7249, Val loss: 1.3443, Val acc: 0.7284, Best Val loss: 1.3443, Best Val acc: 0.7284\n",
            "Epoch: 10/90, Train loss: 0.7807, Train acc: 0.7937, Val loss: 0.5878, Val acc: 0.7531, Best Val loss: 0.5878, Best Val acc: 0.7531\n",
            "Epoch: 11/90, Train loss: 0.5710, Train acc: 0.7884, Val loss: 0.4266, Val acc: 0.8272, Best Val loss: 0.4266, Best Val acc: 0.8272\n",
            "Epoch: 12/90, Train loss: 0.3564, Train acc: 0.8519, Val loss: 1.0935, Val acc: 0.5185, Best Val loss: 0.4266, Best Val acc: 0.8272\n",
            "Epoch: 13/90, Train loss: 0.8138, Train acc: 0.6984, Val loss: 0.8358, Val acc: 0.7531, Best Val loss: 0.4266, Best Val acc: 0.8272\n",
            "Epoch: 14/90, Train loss: 0.5290, Train acc: 0.8148, Val loss: 0.5411, Val acc: 0.7160, Best Val loss: 0.4266, Best Val acc: 0.8272\n",
            "Epoch: 15/90, Train loss: 0.4288, Train acc: 0.8254, Val loss: 0.4674, Val acc: 0.8148, Best Val loss: 0.4266, Best Val acc: 0.8272\n",
            "Epoch: 16/90, Train loss: 0.4500, Train acc: 0.7989, Val loss: 0.4889, Val acc: 0.7531, Best Val loss: 0.4266, Best Val acc: 0.8272\n",
            "Epoch: 17/90, Train loss: 0.3616, Train acc: 0.8042, Val loss: 0.4073, Val acc: 0.8272, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 18/90, Train loss: 0.3451, Train acc: 0.8360, Val loss: 0.4576, Val acc: 0.8272, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 19/90, Train loss: 0.3254, Train acc: 0.8360, Val loss: 0.4478, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 20/90, Train loss: 0.3230, Train acc: 0.8254, Val loss: 0.4358, Val acc: 0.8148, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 21/90, Train loss: 0.3124, Train acc: 0.8519, Val loss: 0.4346, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 22/90, Train loss: 0.2977, Train acc: 0.8519, Val loss: 0.5141, Val acc: 0.8148, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 23/90, Train loss: 0.3118, Train acc: 0.8307, Val loss: 0.4444, Val acc: 0.7654, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 24/90, Train loss: 0.3182, Train acc: 0.8519, Val loss: 0.5391, Val acc: 0.7284, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 25/90, Train loss: 0.2885, Train acc: 0.8519, Val loss: 0.5320, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 26/90, Train loss: 0.2995, Train acc: 0.8677, Val loss: 0.5250, Val acc: 0.7531, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 27/90, Train loss: 0.3133, Train acc: 0.8466, Val loss: 0.4882, Val acc: 0.8272, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 28/90, Train loss: 0.3051, Train acc: 0.8466, Val loss: 0.4892, Val acc: 0.8148, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 29/90, Train loss: 0.3146, Train acc: 0.8254, Val loss: 0.5935, Val acc: 0.8148, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 30/90, Train loss: 0.3309, Train acc: 0.8519, Val loss: 0.5362, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 31/90, Train loss: 0.2829, Train acc: 0.8519, Val loss: 0.5359, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 32/90, Train loss: 0.2585, Train acc: 0.8783, Val loss: 0.5434, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 33/90, Train loss: 0.2545, Train acc: 0.8730, Val loss: 0.5988, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 34/90, Train loss: 0.2596, Train acc: 0.8783, Val loss: 0.5478, Val acc: 0.8272, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 35/90, Train loss: 0.2542, Train acc: 0.8836, Val loss: 0.5982, Val acc: 0.7531, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 36/90, Train loss: 0.2591, Train acc: 0.8836, Val loss: 0.6362, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 37/90, Train loss: 0.2498, Train acc: 0.8836, Val loss: 0.5711, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 38/90, Train loss: 0.2571, Train acc: 0.8836, Val loss: 0.5987, Val acc: 0.7531, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 39/90, Train loss: 0.2410, Train acc: 0.8783, Val loss: 0.5871, Val acc: 0.8395, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 40/90, Train loss: 0.2311, Train acc: 0.8995, Val loss: 0.6414, Val acc: 0.8272, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 41/90, Train loss: 0.2793, Train acc: 0.8730, Val loss: 0.5000, Val acc: 0.8148, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 42/90, Train loss: 0.2631, Train acc: 0.8730, Val loss: 0.5851, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 43/90, Train loss: 0.2402, Train acc: 0.8730, Val loss: 0.5156, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 44/90, Train loss: 0.2926, Train acc: 0.8519, Val loss: 0.5726, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 45/90, Train loss: 0.3005, Train acc: 0.8730, Val loss: 0.6134, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 46/90, Train loss: 0.2516, Train acc: 0.8942, Val loss: 0.6159, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 47/90, Train loss: 0.2481, Train acc: 0.8942, Val loss: 0.6691, Val acc: 0.8148, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 48/90, Train loss: 0.2585, Train acc: 0.8836, Val loss: 0.6132, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 49/90, Train loss: 0.2390, Train acc: 0.8942, Val loss: 0.5984, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 50/90, Train loss: 0.2690, Train acc: 0.8730, Val loss: 0.6042, Val acc: 0.7531, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 51/90, Train loss: 0.2259, Train acc: 0.8995, Val loss: 0.5770, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 52/90, Train loss: 0.2327, Train acc: 0.9101, Val loss: 0.6225, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 53/90, Train loss: 0.2463, Train acc: 0.9101, Val loss: 0.6628, Val acc: 0.7654, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 54/90, Train loss: 0.2525, Train acc: 0.9101, Val loss: 0.6389, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 55/90, Train loss: 0.2321, Train acc: 0.8730, Val loss: 0.5769, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 56/90, Train loss: 0.2207, Train acc: 0.9153, Val loss: 0.6453, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 57/90, Train loss: 0.2597, Train acc: 0.8783, Val loss: 0.5791, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 58/90, Train loss: 0.2609, Train acc: 0.8836, Val loss: 0.5992, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 59/90, Train loss: 0.2234, Train acc: 0.8942, Val loss: 0.6317, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 60/90, Train loss: 0.2025, Train acc: 0.9153, Val loss: 0.6243, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 61/90, Train loss: 0.2343, Train acc: 0.8889, Val loss: 0.7510, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 62/90, Train loss: 0.2326, Train acc: 0.9101, Val loss: 0.6379, Val acc: 0.7654, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 63/90, Train loss: 0.2955, Train acc: 0.8466, Val loss: 0.5704, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 64/90, Train loss: 0.2385, Train acc: 0.8942, Val loss: 0.7280, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 65/90, Train loss: 0.2211, Train acc: 0.8889, Val loss: 0.6322, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 66/90, Train loss: 0.2249, Train acc: 0.8995, Val loss: 0.6929, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 67/90, Train loss: 0.2069, Train acc: 0.9048, Val loss: 0.7557, Val acc: 0.7531, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 68/90, Train loss: 0.2385, Train acc: 0.8783, Val loss: 0.6742, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 69/90, Train loss: 0.2372, Train acc: 0.8942, Val loss: 0.6312, Val acc: 0.8272, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 70/90, Train loss: 0.2326, Train acc: 0.8783, Val loss: 0.6976, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 71/90, Train loss: 0.2056, Train acc: 0.9048, Val loss: 0.6662, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 72/90, Train loss: 0.2101, Train acc: 0.8942, Val loss: 0.7194, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 73/90, Train loss: 0.2341, Train acc: 0.8836, Val loss: 0.5964, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 74/90, Train loss: 0.2312, Train acc: 0.8942, Val loss: 0.7398, Val acc: 0.8148, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 75/90, Train loss: 0.2570, Train acc: 0.8571, Val loss: 0.6874, Val acc: 0.7407, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 76/90, Train loss: 0.2244, Train acc: 0.9048, Val loss: 0.6355, Val acc: 0.7531, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 77/90, Train loss: 0.2217, Train acc: 0.9048, Val loss: 0.6121, Val acc: 0.7654, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 78/90, Train loss: 0.2171, Train acc: 0.8995, Val loss: 0.6426, Val acc: 0.8148, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 79/90, Train loss: 0.1930, Train acc: 0.9259, Val loss: 0.6773, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 80/90, Train loss: 0.1876, Train acc: 0.9259, Val loss: 0.7276, Val acc: 0.7284, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 81/90, Train loss: 0.1868, Train acc: 0.9259, Val loss: 0.7161, Val acc: 0.7407, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 82/90, Train loss: 0.2095, Train acc: 0.9048, Val loss: 0.6720, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 83/90, Train loss: 0.1987, Train acc: 0.9206, Val loss: 0.7137, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 84/90, Train loss: 0.1869, Train acc: 0.9259, Val loss: 0.6646, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 85/90, Train loss: 0.1681, Train acc: 0.9312, Val loss: 0.6792, Val acc: 0.8025, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 86/90, Train loss: 0.1838, Train acc: 0.9101, Val loss: 0.7586, Val acc: 0.7778, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 87/90, Train loss: 0.2120, Train acc: 0.8889, Val loss: 0.6434, Val acc: 0.7901, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 88/90, Train loss: 0.1772, Train acc: 0.9153, Val loss: 0.7336, Val acc: 0.7654, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 89/90, Train loss: 0.1796, Train acc: 0.9312, Val loss: 0.7317, Val acc: 0.7654, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Epoch: 90/90, Train loss: 0.1605, Train acc: 0.9365, Val loss: 0.6939, Val acc: 0.7654, Best Val loss: 0.4073, Best Val acc: 0.8272\n",
            "Test Accuracy for batch size 16, epochs 90: 77.4194%\n",
            "Training with batch size: 16, epochs: 100\n",
            "Epoch: 1/100, Train loss: 0.1628, Train acc: 0.9418, Val loss: 0.7959, Val acc: 0.7654, Best Val loss: 0.7959, Best Val acc: 0.7654\n",
            "Epoch: 2/100, Train loss: 0.1960, Train acc: 0.9312, Val loss: 0.6930, Val acc: 0.7531, Best Val loss: 0.6930, Best Val acc: 0.7531\n",
            "Epoch: 3/100, Train loss: 0.2395, Train acc: 0.8836, Val loss: 0.6283, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 4/100, Train loss: 0.2102, Train acc: 0.9048, Val loss: 0.8060, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 5/100, Train loss: 0.2025, Train acc: 0.9153, Val loss: 0.7819, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 6/100, Train loss: 0.1875, Train acc: 0.9259, Val loss: 0.7690, Val acc: 0.7407, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 7/100, Train loss: 0.1601, Train acc: 0.9471, Val loss: 0.7285, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 8/100, Train loss: 0.1670, Train acc: 0.9418, Val loss: 0.7318, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 9/100, Train loss: 0.1788, Train acc: 0.9365, Val loss: 0.7677, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 10/100, Train loss: 0.2134, Train acc: 0.9048, Val loss: 0.6739, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 11/100, Train loss: 0.2333, Train acc: 0.9153, Val loss: 0.7296, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 12/100, Train loss: 0.1870, Train acc: 0.9206, Val loss: 0.8268, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 13/100, Train loss: 0.1749, Train acc: 0.9312, Val loss: 0.7117, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 14/100, Train loss: 0.1653, Train acc: 0.9365, Val loss: 0.7970, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 15/100, Train loss: 0.1756, Train acc: 0.9312, Val loss: 0.7747, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 16/100, Train loss: 0.1914, Train acc: 0.9048, Val loss: 0.8593, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 17/100, Train loss: 0.1791, Train acc: 0.9312, Val loss: 0.8058, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 18/100, Train loss: 0.1625, Train acc: 0.9312, Val loss: 0.7731, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 19/100, Train loss: 0.1418, Train acc: 0.9418, Val loss: 0.7172, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 20/100, Train loss: 0.1360, Train acc: 0.9524, Val loss: 0.8084, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 21/100, Train loss: 0.1383, Train acc: 0.9365, Val loss: 0.9210, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 22/100, Train loss: 0.1675, Train acc: 0.9418, Val loss: 0.8692, Val acc: 0.7160, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 23/100, Train loss: 0.2426, Train acc: 0.8889, Val loss: 0.9800, Val acc: 0.7407, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 24/100, Train loss: 0.2400, Train acc: 0.8783, Val loss: 0.7418, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 25/100, Train loss: 0.1711, Train acc: 0.9365, Val loss: 0.8257, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 26/100, Train loss: 0.1542, Train acc: 0.9418, Val loss: 0.7652, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 27/100, Train loss: 0.1306, Train acc: 0.9471, Val loss: 0.8010, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 28/100, Train loss: 0.1328, Train acc: 0.9577, Val loss: 0.8781, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 29/100, Train loss: 0.1354, Train acc: 0.9471, Val loss: 0.8492, Val acc: 0.8025, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 30/100, Train loss: 0.1396, Train acc: 0.9524, Val loss: 0.9795, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 31/100, Train loss: 0.1211, Train acc: 0.9524, Val loss: 0.9979, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 32/100, Train loss: 0.1363, Train acc: 0.9365, Val loss: 0.9734, Val acc: 0.7160, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 33/100, Train loss: 0.1894, Train acc: 0.9101, Val loss: 1.0123, Val acc: 0.8148, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 34/100, Train loss: 0.2844, Train acc: 0.8677, Val loss: 0.7431, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 35/100, Train loss: 0.3133, Train acc: 0.8413, Val loss: 0.7916, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 36/100, Train loss: 0.2903, Train acc: 0.8571, Val loss: 0.7473, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 37/100, Train loss: 0.2232, Train acc: 0.8730, Val loss: 0.8576, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 38/100, Train loss: 0.1657, Train acc: 0.9206, Val loss: 0.8375, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 39/100, Train loss: 0.1410, Train acc: 0.9365, Val loss: 0.9550, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 40/100, Train loss: 0.2369, Train acc: 0.9206, Val loss: 0.8332, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 41/100, Train loss: 0.1159, Train acc: 0.9524, Val loss: 0.8492, Val acc: 0.7407, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 42/100, Train loss: 0.1167, Train acc: 0.9471, Val loss: 0.9035, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 43/100, Train loss: 0.1079, Train acc: 0.9577, Val loss: 0.8541, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 44/100, Train loss: 0.0963, Train acc: 0.9577, Val loss: 0.9759, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 45/100, Train loss: 0.0892, Train acc: 0.9630, Val loss: 1.0042, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 46/100, Train loss: 0.0978, Train acc: 0.9630, Val loss: 1.0197, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 47/100, Train loss: 0.0961, Train acc: 0.9630, Val loss: 0.9959, Val acc: 0.7407, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 48/100, Train loss: 0.1326, Train acc: 0.9365, Val loss: 1.0980, Val acc: 0.7407, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 49/100, Train loss: 0.1066, Train acc: 0.9471, Val loss: 1.0227, Val acc: 0.7407, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 50/100, Train loss: 0.0909, Train acc: 0.9630, Val loss: 1.0316, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 51/100, Train loss: 0.0857, Train acc: 0.9788, Val loss: 1.1101, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 52/100, Train loss: 0.0888, Train acc: 0.9630, Val loss: 1.1818, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 53/100, Train loss: 0.1305, Train acc: 0.9418, Val loss: 1.2147, Val acc: 0.7407, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 54/100, Train loss: 0.1103, Train acc: 0.9418, Val loss: 1.1374, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 55/100, Train loss: 0.0882, Train acc: 0.9683, Val loss: 1.2684, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 56/100, Train loss: 0.0836, Train acc: 0.9630, Val loss: 1.1664, Val acc: 0.7407, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 57/100, Train loss: 0.0779, Train acc: 0.9735, Val loss: 1.2021, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 58/100, Train loss: 0.0673, Train acc: 0.9735, Val loss: 1.2912, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 59/100, Train loss: 0.0650, Train acc: 0.9788, Val loss: 1.1815, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 60/100, Train loss: 0.0596, Train acc: 0.9841, Val loss: 1.2843, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 61/100, Train loss: 0.0599, Train acc: 0.9788, Val loss: 1.2290, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 62/100, Train loss: 0.0587, Train acc: 0.9788, Val loss: 1.3189, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 63/100, Train loss: 0.0526, Train acc: 0.9841, Val loss: 1.2584, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 64/100, Train loss: 0.0504, Train acc: 0.9788, Val loss: 1.3975, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 65/100, Train loss: 0.0801, Train acc: 0.9630, Val loss: 1.2329, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 66/100, Train loss: 0.1082, Train acc: 0.9683, Val loss: 1.2361, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 67/100, Train loss: 0.1192, Train acc: 0.9577, Val loss: 1.2323, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 68/100, Train loss: 0.0832, Train acc: 0.9577, Val loss: 1.2983, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 69/100, Train loss: 0.0618, Train acc: 0.9735, Val loss: 1.3977, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 70/100, Train loss: 0.0800, Train acc: 0.9735, Val loss: 1.3252, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 71/100, Train loss: 0.0639, Train acc: 0.9630, Val loss: 1.3099, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 72/100, Train loss: 0.0655, Train acc: 0.9683, Val loss: 1.3351, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 73/100, Train loss: 0.0936, Train acc: 0.9471, Val loss: 1.2703, Val acc: 0.7160, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 74/100, Train loss: 0.2717, Train acc: 0.8836, Val loss: 1.3765, Val acc: 0.7284, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 75/100, Train loss: 0.3530, Train acc: 0.8730, Val loss: 1.1104, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 76/100, Train loss: 0.5714, Train acc: 0.7619, Val loss: 0.9278, Val acc: 0.7037, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 77/100, Train loss: 0.3621, Train acc: 0.8360, Val loss: 0.8123, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 78/100, Train loss: 0.3213, Train acc: 0.8624, Val loss: 1.1672, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 79/100, Train loss: 0.3677, Train acc: 0.8519, Val loss: 0.7742, Val acc: 0.8148, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 80/100, Train loss: 0.2310, Train acc: 0.8836, Val loss: 0.8829, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 81/100, Train loss: 0.1963, Train acc: 0.9153, Val loss: 0.9132, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 82/100, Train loss: 0.1562, Train acc: 0.9153, Val loss: 1.0268, Val acc: 0.8272, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 83/100, Train loss: 0.1359, Train acc: 0.9418, Val loss: 1.0247, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 84/100, Train loss: 0.1472, Train acc: 0.9312, Val loss: 1.2088, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 85/100, Train loss: 0.1686, Train acc: 0.9101, Val loss: 0.9302, Val acc: 0.8025, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 86/100, Train loss: 0.1257, Train acc: 0.9577, Val loss: 1.0219, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 87/100, Train loss: 0.1913, Train acc: 0.9101, Val loss: 1.1977, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 88/100, Train loss: 0.1319, Train acc: 0.9418, Val loss: 1.2018, Val acc: 0.7160, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 89/100, Train loss: 0.1892, Train acc: 0.8995, Val loss: 1.2901, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 90/100, Train loss: 0.3104, Train acc: 0.8783, Val loss: 0.9616, Val acc: 0.7778, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 91/100, Train loss: 0.2353, Train acc: 0.8624, Val loss: 0.8668, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 92/100, Train loss: 0.1729, Train acc: 0.9206, Val loss: 0.9605, Val acc: 0.8272, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 93/100, Train loss: 0.1212, Train acc: 0.9471, Val loss: 1.1713, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 94/100, Train loss: 0.1212, Train acc: 0.9683, Val loss: 1.0967, Val acc: 0.8148, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 95/100, Train loss: 0.1075, Train acc: 0.9471, Val loss: 1.0625, Val acc: 0.7654, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 96/100, Train loss: 0.1071, Train acc: 0.9577, Val loss: 1.1150, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 97/100, Train loss: 0.0929, Train acc: 0.9577, Val loss: 1.2011, Val acc: 0.8025, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 98/100, Train loss: 0.0896, Train acc: 0.9471, Val loss: 1.1617, Val acc: 0.8025, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 99/100, Train loss: 0.0776, Train acc: 0.9683, Val loss: 1.2374, Val acc: 0.7901, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Epoch: 100/100, Train loss: 0.0914, Train acc: 0.9524, Val loss: 1.1266, Val acc: 0.7531, Best Val loss: 0.6283, Best Val acc: 0.7778\n",
            "Test Accuracy for batch size 16, epochs 100: 70.9677%\n",
            "Training with batch size: 16, epochs: 110\n",
            "Epoch: 1/110, Train loss: 0.0846, Train acc: 0.9524, Val loss: 1.2145, Val acc: 0.8148, Best Val loss: 1.2145, Best Val acc: 0.8148\n",
            "Epoch: 2/110, Train loss: 0.0801, Train acc: 0.9577, Val loss: 1.1786, Val acc: 0.7531, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 3/110, Train loss: 0.0609, Train acc: 0.9788, Val loss: 1.2497, Val acc: 0.7654, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 4/110, Train loss: 0.0478, Train acc: 0.9894, Val loss: 1.2108, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 5/110, Train loss: 0.0412, Train acc: 0.9947, Val loss: 1.2544, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 6/110, Train loss: 0.0387, Train acc: 0.9894, Val loss: 1.2540, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 7/110, Train loss: 0.0394, Train acc: 0.9894, Val loss: 1.2552, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 8/110, Train loss: 0.0424, Train acc: 0.9788, Val loss: 1.3282, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 9/110, Train loss: 0.0549, Train acc: 0.9841, Val loss: 1.2675, Val acc: 0.7654, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 10/110, Train loss: 0.0358, Train acc: 0.9947, Val loss: 1.3638, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 11/110, Train loss: 0.0320, Train acc: 0.9894, Val loss: 1.3356, Val acc: 0.8025, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 12/110, Train loss: 0.0341, Train acc: 0.9894, Val loss: 1.3934, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 13/110, Train loss: 0.0299, Train acc: 0.9947, Val loss: 1.4124, Val acc: 0.8025, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 14/110, Train loss: 0.0292, Train acc: 0.9894, Val loss: 1.4341, Val acc: 0.8025, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 15/110, Train loss: 0.0271, Train acc: 0.9947, Val loss: 1.4096, Val acc: 0.7654, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 16/110, Train loss: 0.0301, Train acc: 0.9947, Val loss: 1.4512, Val acc: 0.8148, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 17/110, Train loss: 0.0349, Train acc: 0.9841, Val loss: 1.4183, Val acc: 0.7654, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 18/110, Train loss: 0.0270, Train acc: 0.9947, Val loss: 1.4209, Val acc: 0.8148, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 19/110, Train loss: 0.0315, Train acc: 0.9894, Val loss: 1.4779, Val acc: 0.8025, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 20/110, Train loss: 0.0278, Train acc: 0.9947, Val loss: 1.4297, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 21/110, Train loss: 0.0267, Train acc: 0.9947, Val loss: 1.4786, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 22/110, Train loss: 0.0228, Train acc: 0.9947, Val loss: 1.4324, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 23/110, Train loss: 0.0242, Train acc: 0.9894, Val loss: 1.4330, Val acc: 0.8148, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 24/110, Train loss: 0.0209, Train acc: 0.9947, Val loss: 1.4150, Val acc: 0.7531, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 25/110, Train loss: 0.0203, Train acc: 1.0000, Val loss: 1.4823, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 26/110, Train loss: 0.0198, Train acc: 0.9947, Val loss: 1.5032, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 27/110, Train loss: 0.0231, Train acc: 0.9947, Val loss: 1.4653, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 28/110, Train loss: 0.0166, Train acc: 1.0000, Val loss: 1.4662, Val acc: 0.8272, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 29/110, Train loss: 0.0177, Train acc: 0.9947, Val loss: 1.5107, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 30/110, Train loss: 0.0158, Train acc: 1.0000, Val loss: 1.5119, Val acc: 0.8148, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 31/110, Train loss: 0.0165, Train acc: 1.0000, Val loss: 1.5044, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 32/110, Train loss: 0.0167, Train acc: 1.0000, Val loss: 1.5100, Val acc: 0.8025, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 33/110, Train loss: 0.0178, Train acc: 1.0000, Val loss: 1.5081, Val acc: 0.8148, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 34/110, Train loss: 0.0177, Train acc: 0.9947, Val loss: 1.5186, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 35/110, Train loss: 0.0280, Train acc: 0.9947, Val loss: 1.4939, Val acc: 0.8272, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 36/110, Train loss: 0.0203, Train acc: 0.9947, Val loss: 1.4716, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 37/110, Train loss: 0.0193, Train acc: 0.9947, Val loss: 1.4751, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 38/110, Train loss: 0.0161, Train acc: 1.0000, Val loss: 1.5934, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 39/110, Train loss: 0.0169, Train acc: 0.9947, Val loss: 1.5434, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 40/110, Train loss: 0.0222, Train acc: 0.9894, Val loss: 1.5376, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 41/110, Train loss: 0.0224, Train acc: 0.9894, Val loss: 1.5494, Val acc: 0.8025, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 42/110, Train loss: 0.0278, Train acc: 0.9947, Val loss: 1.5592, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 43/110, Train loss: 0.0280, Train acc: 0.9947, Val loss: 1.6142, Val acc: 0.7654, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 44/110, Train loss: 0.0330, Train acc: 0.9841, Val loss: 1.4676, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 45/110, Train loss: 0.0321, Train acc: 0.9894, Val loss: 1.3745, Val acc: 0.7654, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 46/110, Train loss: 0.0473, Train acc: 0.9894, Val loss: 1.6184, Val acc: 0.7531, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 47/110, Train loss: 0.1975, Train acc: 0.9259, Val loss: 1.4835, Val acc: 0.7778, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 48/110, Train loss: 0.5463, Train acc: 0.8466, Val loss: 1.5025, Val acc: 0.7901, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 49/110, Train loss: 0.9974, Train acc: 0.8466, Val loss: 1.4390, Val acc: 0.7407, Best Val loss: 1.1786, Best Val acc: 0.7531\n",
            "Epoch: 50/110, Train loss: 0.4662, Train acc: 0.8624, Val loss: 0.6889, Val acc: 0.7407, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 51/110, Train loss: 0.3012, Train acc: 0.8254, Val loss: 0.7435, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 52/110, Train loss: 0.3314, Train acc: 0.8148, Val loss: 0.8619, Val acc: 0.7531, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 53/110, Train loss: 0.2199, Train acc: 0.8942, Val loss: 0.7447, Val acc: 0.7778, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 54/110, Train loss: 0.1591, Train acc: 0.9206, Val loss: 0.7909, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 55/110, Train loss: 0.2275, Train acc: 0.9101, Val loss: 0.8971, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 56/110, Train loss: 0.1551, Train acc: 0.9524, Val loss: 0.7794, Val acc: 0.8272, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 57/110, Train loss: 0.1348, Train acc: 0.9577, Val loss: 0.8908, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 58/110, Train loss: 0.0988, Train acc: 0.9735, Val loss: 0.9153, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 59/110, Train loss: 0.0926, Train acc: 0.9735, Val loss: 0.9097, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 60/110, Train loss: 0.0816, Train acc: 0.9735, Val loss: 0.9518, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 61/110, Train loss: 0.0737, Train acc: 0.9841, Val loss: 0.9780, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 62/110, Train loss: 0.0684, Train acc: 0.9841, Val loss: 1.0354, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 63/110, Train loss: 0.0752, Train acc: 0.9788, Val loss: 1.0185, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 64/110, Train loss: 0.0729, Train acc: 0.9841, Val loss: 1.0949, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 65/110, Train loss: 0.0670, Train acc: 0.9841, Val loss: 1.1156, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 66/110, Train loss: 0.0601, Train acc: 0.9841, Val loss: 1.1555, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 67/110, Train loss: 0.0589, Train acc: 0.9788, Val loss: 1.1602, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 68/110, Train loss: 0.0563, Train acc: 0.9841, Val loss: 1.1507, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 69/110, Train loss: 0.0584, Train acc: 0.9841, Val loss: 1.1517, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 70/110, Train loss: 0.0515, Train acc: 0.9841, Val loss: 1.1405, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 71/110, Train loss: 0.0558, Train acc: 0.9841, Val loss: 1.1590, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 72/110, Train loss: 0.0486, Train acc: 0.9841, Val loss: 1.1793, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 73/110, Train loss: 0.0487, Train acc: 0.9841, Val loss: 1.2154, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 74/110, Train loss: 0.0475, Train acc: 0.9841, Val loss: 1.2151, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 75/110, Train loss: 0.0491, Train acc: 0.9841, Val loss: 1.2693, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 76/110, Train loss: 0.0488, Train acc: 0.9841, Val loss: 1.2408, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 77/110, Train loss: 0.0465, Train acc: 0.9841, Val loss: 1.2561, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 78/110, Train loss: 0.0427, Train acc: 0.9841, Val loss: 1.2636, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 79/110, Train loss: 0.0429, Train acc: 0.9841, Val loss: 1.3143, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 80/110, Train loss: 0.0419, Train acc: 0.9841, Val loss: 1.2857, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 81/110, Train loss: 0.0440, Train acc: 0.9841, Val loss: 1.3394, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 82/110, Train loss: 0.0413, Train acc: 0.9841, Val loss: 1.4003, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 83/110, Train loss: 0.0387, Train acc: 0.9841, Val loss: 1.3731, Val acc: 0.7778, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 84/110, Train loss: 0.0419, Train acc: 0.9841, Val loss: 1.3302, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 85/110, Train loss: 0.0396, Train acc: 0.9841, Val loss: 1.3302, Val acc: 0.7778, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 86/110, Train loss: 0.0381, Train acc: 0.9841, Val loss: 1.4114, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 87/110, Train loss: 0.0346, Train acc: 0.9841, Val loss: 1.4030, Val acc: 0.7778, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 88/110, Train loss: 0.0364, Train acc: 0.9841, Val loss: 1.4247, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 89/110, Train loss: 0.0319, Train acc: 0.9841, Val loss: 1.4380, Val acc: 0.7778, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 90/110, Train loss: 0.0327, Train acc: 0.9841, Val loss: 1.4873, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 91/110, Train loss: 0.0311, Train acc: 0.9841, Val loss: 1.4783, Val acc: 0.7778, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 92/110, Train loss: 0.0317, Train acc: 0.9841, Val loss: 1.4773, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 93/110, Train loss: 0.0300, Train acc: 0.9841, Val loss: 1.5269, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 94/110, Train loss: 0.0279, Train acc: 0.9841, Val loss: 1.5523, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 95/110, Train loss: 0.0281, Train acc: 0.9841, Val loss: 1.5610, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 96/110, Train loss: 0.0280, Train acc: 0.9841, Val loss: 1.5661, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 97/110, Train loss: 0.0331, Train acc: 0.9841, Val loss: 1.5363, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 98/110, Train loss: 0.0284, Train acc: 0.9841, Val loss: 1.5412, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 99/110, Train loss: 0.0318, Train acc: 0.9841, Val loss: 1.5752, Val acc: 0.7778, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 100/110, Train loss: 0.0301, Train acc: 1.0000, Val loss: 1.5556, Val acc: 0.7778, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 101/110, Train loss: 0.0304, Train acc: 0.9947, Val loss: 1.6376, Val acc: 0.7778, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 102/110, Train loss: 0.0245, Train acc: 0.9841, Val loss: 1.6410, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 103/110, Train loss: 0.0250, Train acc: 0.9894, Val loss: 1.6286, Val acc: 0.7778, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 104/110, Train loss: 0.0236, Train acc: 0.9947, Val loss: 1.6759, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 105/110, Train loss: 0.0248, Train acc: 0.9894, Val loss: 1.6618, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 106/110, Train loss: 0.0235, Train acc: 0.9894, Val loss: 1.6216, Val acc: 0.7901, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 107/110, Train loss: 0.0210, Train acc: 1.0000, Val loss: 1.6719, Val acc: 0.8148, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 108/110, Train loss: 0.0195, Train acc: 1.0000, Val loss: 1.6808, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 109/110, Train loss: 0.0213, Train acc: 0.9947, Val loss: 1.6764, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Epoch: 110/110, Train loss: 0.0189, Train acc: 0.9947, Val loss: 1.7251, Val acc: 0.8025, Best Val loss: 0.6889, Best Val acc: 0.7407\n",
            "Test Accuracy for batch size 16, epochs 110: 80.6452%\n",
            "Training with batch size: 32, epochs: 90\n",
            "Epoch: 1/90, Train loss: 0.0193, Train acc: 0.9947, Val loss: 1.7158, Val acc: 0.8148, Best Val loss: 1.7158, Best Val acc: 0.8148\n",
            "Epoch: 2/90, Train loss: 0.0179, Train acc: 1.0000, Val loss: 1.6969, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 3/90, Train loss: 0.0174, Train acc: 1.0000, Val loss: 1.7170, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 4/90, Train loss: 0.0171, Train acc: 1.0000, Val loss: 1.7349, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 5/90, Train loss: 0.0161, Train acc: 1.0000, Val loss: 1.7590, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 6/90, Train loss: 0.0160, Train acc: 1.0000, Val loss: 1.7690, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 7/90, Train loss: 0.0168, Train acc: 1.0000, Val loss: 1.7810, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 8/90, Train loss: 0.0157, Train acc: 1.0000, Val loss: 1.7739, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 9/90, Train loss: 0.0154, Train acc: 1.0000, Val loss: 1.7741, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 10/90, Train loss: 0.0155, Train acc: 1.0000, Val loss: 1.7970, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 11/90, Train loss: 0.0151, Train acc: 1.0000, Val loss: 1.7955, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 12/90, Train loss: 0.0148, Train acc: 1.0000, Val loss: 1.7917, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 13/90, Train loss: 0.0145, Train acc: 1.0000, Val loss: 1.7925, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 14/90, Train loss: 0.0142, Train acc: 1.0000, Val loss: 1.7978, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 15/90, Train loss: 0.0139, Train acc: 1.0000, Val loss: 1.7996, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 16/90, Train loss: 0.0141, Train acc: 1.0000, Val loss: 1.8176, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 17/90, Train loss: 0.0135, Train acc: 1.0000, Val loss: 1.8339, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 18/90, Train loss: 0.0129, Train acc: 1.0000, Val loss: 1.8351, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 19/90, Train loss: 0.0136, Train acc: 1.0000, Val loss: 1.8342, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 20/90, Train loss: 0.0130, Train acc: 1.0000, Val loss: 1.8596, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 21/90, Train loss: 0.0124, Train acc: 1.0000, Val loss: 1.8696, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 22/90, Train loss: 0.0122, Train acc: 1.0000, Val loss: 1.8781, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 23/90, Train loss: 0.0123, Train acc: 1.0000, Val loss: 1.8619, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 24/90, Train loss: 0.0122, Train acc: 1.0000, Val loss: 1.8550, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 25/90, Train loss: 0.0115, Train acc: 1.0000, Val loss: 1.8643, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 26/90, Train loss: 0.0114, Train acc: 1.0000, Val loss: 1.8787, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 27/90, Train loss: 0.0108, Train acc: 1.0000, Val loss: 1.8805, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 28/90, Train loss: 0.0105, Train acc: 1.0000, Val loss: 1.9100, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 29/90, Train loss: 0.0112, Train acc: 1.0000, Val loss: 1.9012, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 30/90, Train loss: 0.0105, Train acc: 1.0000, Val loss: 1.8882, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 31/90, Train loss: 0.0105, Train acc: 1.0000, Val loss: 1.9146, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 32/90, Train loss: 0.0099, Train acc: 1.0000, Val loss: 1.9524, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 33/90, Train loss: 0.0099, Train acc: 1.0000, Val loss: 1.9558, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 34/90, Train loss: 0.0093, Train acc: 1.0000, Val loss: 1.9586, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 35/90, Train loss: 0.0098, Train acc: 1.0000, Val loss: 1.9669, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 36/90, Train loss: 0.0097, Train acc: 1.0000, Val loss: 1.9785, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 37/90, Train loss: 0.0094, Train acc: 1.0000, Val loss: 1.9764, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 38/90, Train loss: 0.0096, Train acc: 1.0000, Val loss: 1.9605, Val acc: 0.8148, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 39/90, Train loss: 0.0091, Train acc: 1.0000, Val loss: 1.9507, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 40/90, Train loss: 0.0083, Train acc: 1.0000, Val loss: 1.9603, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 41/90, Train loss: 0.0095, Train acc: 1.0000, Val loss: 1.9525, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 42/90, Train loss: 0.0087, Train acc: 1.0000, Val loss: 1.9486, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 43/90, Train loss: 0.0093, Train acc: 1.0000, Val loss: 1.9600, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 44/90, Train loss: 0.0089, Train acc: 1.0000, Val loss: 1.9969, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 45/90, Train loss: 0.0079, Train acc: 1.0000, Val loss: 1.9907, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 46/90, Train loss: 0.0081, Train acc: 1.0000, Val loss: 1.9783, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 47/90, Train loss: 0.0089, Train acc: 1.0000, Val loss: 2.0254, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 48/90, Train loss: 0.0083, Train acc: 1.0000, Val loss: 2.0277, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 49/90, Train loss: 0.0075, Train acc: 1.0000, Val loss: 2.0078, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 50/90, Train loss: 0.0076, Train acc: 1.0000, Val loss: 2.0042, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 51/90, Train loss: 0.0070, Train acc: 1.0000, Val loss: 2.0044, Val acc: 0.7654, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 52/90, Train loss: 0.0069, Train acc: 1.0000, Val loss: 2.0361, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 53/90, Train loss: 0.0069, Train acc: 1.0000, Val loss: 2.0516, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 54/90, Train loss: 0.0066, Train acc: 1.0000, Val loss: 2.0568, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 55/90, Train loss: 0.0068, Train acc: 1.0000, Val loss: 2.0189, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 56/90, Train loss: 0.0066, Train acc: 1.0000, Val loss: 2.0323, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 57/90, Train loss: 0.0066, Train acc: 1.0000, Val loss: 2.0543, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 58/90, Train loss: 0.0062, Train acc: 1.0000, Val loss: 2.0627, Val acc: 0.8148, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 59/90, Train loss: 0.0058, Train acc: 1.0000, Val loss: 2.0829, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 60/90, Train loss: 0.0059, Train acc: 1.0000, Val loss: 2.0716, Val acc: 0.7654, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 61/90, Train loss: 0.0060, Train acc: 1.0000, Val loss: 2.0809, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 62/90, Train loss: 0.0059, Train acc: 1.0000, Val loss: 2.0757, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 63/90, Train loss: 0.0059, Train acc: 1.0000, Val loss: 2.0831, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 64/90, Train loss: 0.0061, Train acc: 1.0000, Val loss: 2.0740, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 65/90, Train loss: 0.0055, Train acc: 1.0000, Val loss: 2.0525, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 66/90, Train loss: 0.0057, Train acc: 1.0000, Val loss: 2.0731, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 67/90, Train loss: 0.0054, Train acc: 1.0000, Val loss: 2.0940, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 68/90, Train loss: 0.0053, Train acc: 1.0000, Val loss: 2.1133, Val acc: 0.8148, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 69/90, Train loss: 0.0052, Train acc: 1.0000, Val loss: 2.1038, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 70/90, Train loss: 0.0051, Train acc: 1.0000, Val loss: 2.0937, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 71/90, Train loss: 0.0053, Train acc: 1.0000, Val loss: 2.0906, Val acc: 0.8148, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 72/90, Train loss: 0.0049, Train acc: 1.0000, Val loss: 2.1029, Val acc: 0.8272, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 73/90, Train loss: 0.0046, Train acc: 1.0000, Val loss: 2.1269, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 74/90, Train loss: 0.0049, Train acc: 1.0000, Val loss: 2.1114, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 75/90, Train loss: 0.0046, Train acc: 1.0000, Val loss: 2.1036, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 76/90, Train loss: 0.0045, Train acc: 1.0000, Val loss: 2.1235, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 77/90, Train loss: 0.0044, Train acc: 1.0000, Val loss: 2.1411, Val acc: 0.8148, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 78/90, Train loss: 0.0042, Train acc: 1.0000, Val loss: 2.1614, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 79/90, Train loss: 0.0043, Train acc: 1.0000, Val loss: 2.1614, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 80/90, Train loss: 0.0043, Train acc: 1.0000, Val loss: 2.1456, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 81/90, Train loss: 0.0042, Train acc: 1.0000, Val loss: 2.1677, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 82/90, Train loss: 0.0044, Train acc: 1.0000, Val loss: 2.1480, Val acc: 0.8272, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 83/90, Train loss: 0.0040, Train acc: 1.0000, Val loss: 2.1465, Val acc: 0.8148, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 84/90, Train loss: 0.0041, Train acc: 1.0000, Val loss: 2.1377, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 85/90, Train loss: 0.0039, Train acc: 1.0000, Val loss: 2.1336, Val acc: 0.7778, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 86/90, Train loss: 0.0041, Train acc: 1.0000, Val loss: 2.1568, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 87/90, Train loss: 0.0038, Train acc: 1.0000, Val loss: 2.1658, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 88/90, Train loss: 0.0036, Train acc: 1.0000, Val loss: 2.1813, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 89/90, Train loss: 0.0038, Train acc: 1.0000, Val loss: 2.1840, Val acc: 0.7901, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Epoch: 90/90, Train loss: 0.0036, Train acc: 1.0000, Val loss: 2.1771, Val acc: 0.8025, Best Val loss: 1.6969, Best Val acc: 0.7901\n",
            "Test Accuracy for batch size 32, epochs 90: 83.8710%\n",
            "Training with batch size: 32, epochs: 100\n",
            "Epoch: 1/100, Train loss: 0.0037, Train acc: 1.0000, Val loss: 2.1827, Val acc: 0.8272, Best Val loss: 2.1827, Best Val acc: 0.8272\n",
            "Epoch: 2/100, Train loss: 0.0037, Train acc: 1.0000, Val loss: 2.1720, Val acc: 0.8272, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 3/100, Train loss: 0.0035, Train acc: 1.0000, Val loss: 2.1750, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 4/100, Train loss: 0.0034, Train acc: 1.0000, Val loss: 2.1794, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 5/100, Train loss: 0.0034, Train acc: 1.0000, Val loss: 2.1777, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 6/100, Train loss: 0.0033, Train acc: 1.0000, Val loss: 2.1887, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 7/100, Train loss: 0.0033, Train acc: 1.0000, Val loss: 2.2092, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 8/100, Train loss: 0.0032, Train acc: 1.0000, Val loss: 2.2270, Val acc: 0.8148, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 9/100, Train loss: 0.0033, Train acc: 1.0000, Val loss: 2.2179, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 10/100, Train loss: 0.0031, Train acc: 1.0000, Val loss: 2.2164, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 11/100, Train loss: 0.0031, Train acc: 1.0000, Val loss: 2.2268, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 12/100, Train loss: 0.0031, Train acc: 1.0000, Val loss: 2.2347, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 13/100, Train loss: 0.0030, Train acc: 1.0000, Val loss: 2.2237, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 14/100, Train loss: 0.0030, Train acc: 1.0000, Val loss: 2.2216, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 15/100, Train loss: 0.0030, Train acc: 1.0000, Val loss: 2.2172, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 16/100, Train loss: 0.0029, Train acc: 1.0000, Val loss: 2.2248, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 17/100, Train loss: 0.0029, Train acc: 1.0000, Val loss: 2.2449, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 18/100, Train loss: 0.0029, Train acc: 1.0000, Val loss: 2.2456, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 19/100, Train loss: 0.0028, Train acc: 1.0000, Val loss: 2.2522, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 20/100, Train loss: 0.0027, Train acc: 1.0000, Val loss: 2.2471, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 21/100, Train loss: 0.0027, Train acc: 1.0000, Val loss: 2.2493, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 22/100, Train loss: 0.0027, Train acc: 1.0000, Val loss: 2.2661, Val acc: 0.8148, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 23/100, Train loss: 0.0027, Train acc: 1.0000, Val loss: 2.2548, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 24/100, Train loss: 0.0027, Train acc: 1.0000, Val loss: 2.2482, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 25/100, Train loss: 0.0027, Train acc: 1.0000, Val loss: 2.2510, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 26/100, Train loss: 0.0026, Train acc: 1.0000, Val loss: 2.2677, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 27/100, Train loss: 0.0026, Train acc: 1.0000, Val loss: 2.2760, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 28/100, Train loss: 0.0026, Train acc: 1.0000, Val loss: 2.2692, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 29/100, Train loss: 0.0025, Train acc: 1.0000, Val loss: 2.2621, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 30/100, Train loss: 0.0025, Train acc: 1.0000, Val loss: 2.2737, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 31/100, Train loss: 0.0024, Train acc: 1.0000, Val loss: 2.2895, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 32/100, Train loss: 0.0024, Train acc: 1.0000, Val loss: 2.2918, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 33/100, Train loss: 0.0024, Train acc: 1.0000, Val loss: 2.2968, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 34/100, Train loss: 0.0024, Train acc: 1.0000, Val loss: 2.2964, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 35/100, Train loss: 0.0024, Train acc: 1.0000, Val loss: 2.2989, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 36/100, Train loss: 0.0024, Train acc: 1.0000, Val loss: 2.2958, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 37/100, Train loss: 0.0023, Train acc: 1.0000, Val loss: 2.2896, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 38/100, Train loss: 0.0023, Train acc: 1.0000, Val loss: 2.2956, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 39/100, Train loss: 0.0023, Train acc: 1.0000, Val loss: 2.3177, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 40/100, Train loss: 0.0022, Train acc: 1.0000, Val loss: 2.3190, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 41/100, Train loss: 0.0022, Train acc: 1.0000, Val loss: 2.3246, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 42/100, Train loss: 0.0023, Train acc: 1.0000, Val loss: 2.3295, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 43/100, Train loss: 0.0021, Train acc: 1.0000, Val loss: 2.3261, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 44/100, Train loss: 0.0022, Train acc: 1.0000, Val loss: 2.3218, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 45/100, Train loss: 0.0021, Train acc: 1.0000, Val loss: 2.3228, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 46/100, Train loss: 0.0021, Train acc: 1.0000, Val loss: 2.3346, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 47/100, Train loss: 0.0021, Train acc: 1.0000, Val loss: 2.3363, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 48/100, Train loss: 0.0021, Train acc: 1.0000, Val loss: 2.3444, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 49/100, Train loss: 0.0021, Train acc: 1.0000, Val loss: 2.3296, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 50/100, Train loss: 0.0020, Train acc: 1.0000, Val loss: 2.3250, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 51/100, Train loss: 0.0020, Train acc: 1.0000, Val loss: 2.3385, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 52/100, Train loss: 0.0021, Train acc: 1.0000, Val loss: 2.3483, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 53/100, Train loss: 0.0021, Train acc: 1.0000, Val loss: 2.3520, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 54/100, Train loss: 0.0020, Train acc: 1.0000, Val loss: 2.3543, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 55/100, Train loss: 0.0019, Train acc: 1.0000, Val loss: 2.3442, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 56/100, Train loss: 0.0020, Train acc: 1.0000, Val loss: 2.3435, Val acc: 0.8025, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 57/100, Train loss: 0.0019, Train acc: 1.0000, Val loss: 2.3521, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 58/100, Train loss: 0.0018, Train acc: 1.0000, Val loss: 2.3692, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 59/100, Train loss: 0.0018, Train acc: 1.0000, Val loss: 2.3653, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 60/100, Train loss: 0.0018, Train acc: 1.0000, Val loss: 2.3666, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 61/100, Train loss: 0.0018, Train acc: 1.0000, Val loss: 2.3688, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 62/100, Train loss: 0.0018, Train acc: 1.0000, Val loss: 2.3747, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 63/100, Train loss: 0.0018, Train acc: 1.0000, Val loss: 2.3833, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 64/100, Train loss: 0.0017, Train acc: 1.0000, Val loss: 2.3687, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 65/100, Train loss: 0.0018, Train acc: 1.0000, Val loss: 2.3659, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 66/100, Train loss: 0.0018, Train acc: 1.0000, Val loss: 2.3735, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 67/100, Train loss: 0.0018, Train acc: 1.0000, Val loss: 2.3807, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 68/100, Train loss: 0.0017, Train acc: 1.0000, Val loss: 2.3982, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 69/100, Train loss: 0.0017, Train acc: 1.0000, Val loss: 2.4065, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 70/100, Train loss: 0.0017, Train acc: 1.0000, Val loss: 2.3891, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 71/100, Train loss: 0.0016, Train acc: 1.0000, Val loss: 2.3863, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 72/100, Train loss: 0.0017, Train acc: 1.0000, Val loss: 2.3986, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 73/100, Train loss: 0.0016, Train acc: 1.0000, Val loss: 2.3860, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 74/100, Train loss: 0.0016, Train acc: 1.0000, Val loss: 2.3949, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 75/100, Train loss: 0.0016, Train acc: 1.0000, Val loss: 2.4035, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 76/100, Train loss: 0.0016, Train acc: 1.0000, Val loss: 2.4126, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 77/100, Train loss: 0.0015, Train acc: 1.0000, Val loss: 2.4164, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 78/100, Train loss: 0.0016, Train acc: 1.0000, Val loss: 2.4180, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 79/100, Train loss: 0.0016, Train acc: 1.0000, Val loss: 2.4217, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 80/100, Train loss: 0.0015, Train acc: 1.0000, Val loss: 2.4116, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 81/100, Train loss: 0.0015, Train acc: 1.0000, Val loss: 2.4070, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 82/100, Train loss: 0.0015, Train acc: 1.0000, Val loss: 2.4155, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 83/100, Train loss: 0.0015, Train acc: 1.0000, Val loss: 2.4063, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 84/100, Train loss: 0.0015, Train acc: 1.0000, Val loss: 2.4068, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 85/100, Train loss: 0.0015, Train acc: 1.0000, Val loss: 2.4097, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 86/100, Train loss: 0.0014, Train acc: 1.0000, Val loss: 2.4236, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 87/100, Train loss: 0.0014, Train acc: 1.0000, Val loss: 2.4366, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 88/100, Train loss: 0.0014, Train acc: 1.0000, Val loss: 2.4383, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 89/100, Train loss: 0.0014, Train acc: 1.0000, Val loss: 2.4441, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 90/100, Train loss: 0.0014, Train acc: 1.0000, Val loss: 2.4465, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 91/100, Train loss: 0.0014, Train acc: 1.0000, Val loss: 2.4340, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 92/100, Train loss: 0.0014, Train acc: 1.0000, Val loss: 2.4342, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 93/100, Train loss: 0.0014, Train acc: 1.0000, Val loss: 2.4457, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 94/100, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.4469, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 95/100, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.4480, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 96/100, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.4567, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 97/100, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.4637, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 98/100, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.4462, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 99/100, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.4519, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Epoch: 100/100, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.4578, Val acc: 0.7901, Best Val loss: 2.1720, Best Val acc: 0.8272\n",
            "Test Accuracy for batch size 32, epochs 100: 83.8710%\n",
            "Training with batch size: 32, epochs: 110\n",
            "Epoch: 1/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4602, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 2/110, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.4692, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 3/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4644, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 4/110, Train loss: 0.0013, Train acc: 1.0000, Val loss: 2.4723, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 5/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4663, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 6/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4687, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 7/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4741, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 8/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4773, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 9/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4641, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 10/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4729, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 11/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4852, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 12/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4737, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 13/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.4728, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 14/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.4804, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 15/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.4909, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 16/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.4823, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 17/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.4676, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 18/110, Train loss: 0.0012, Train acc: 1.0000, Val loss: 2.4793, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 19/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.4978, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 20/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.5119, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 21/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.5177, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 22/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.5094, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 23/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5042, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 24/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.5070, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 25/110, Train loss: 0.0011, Train acc: 1.0000, Val loss: 2.5123, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 26/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5068, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 27/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5162, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 28/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5305, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 29/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5188, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 30/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5063, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 31/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5100, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 32/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5168, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 33/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5287, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 34/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5380, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 35/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5399, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 36/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5425, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 37/110, Train loss: 0.0010, Train acc: 1.0000, Val loss: 2.5402, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 38/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5391, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 39/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5436, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 40/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5479, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 41/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5502, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 42/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5449, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 43/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5459, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 44/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5502, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 45/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5504, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 46/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5530, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 47/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5611, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 48/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5502, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 49/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5531, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 50/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5654, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 51/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5664, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 52/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5532, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 53/110, Train loss: 0.0009, Train acc: 1.0000, Val loss: 2.5540, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 54/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5625, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 55/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5715, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 56/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5778, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 57/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5817, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 58/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5783, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 59/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5804, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 60/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5767, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 61/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5651, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 62/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5643, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 63/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5777, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 64/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5911, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 65/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5882, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 66/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5869, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 67/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5927, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 68/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5966, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 69/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6071, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 70/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6033, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 71/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6003, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 72/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6015, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 73/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6066, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 74/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.5996, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 75/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.5775, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 76/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5800, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 77/110, Train loss: 0.0008, Train acc: 1.0000, Val loss: 2.5945, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 78/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6107, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 79/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6199, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 80/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6274, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 81/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6375, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 82/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6349, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 83/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6278, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 84/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6176, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 85/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6228, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 86/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6376, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 87/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6241, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 88/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6035, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 89/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6106, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 90/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6210, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 91/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6283, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 92/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6406, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 93/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6506, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 94/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6563, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 95/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6543, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 96/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6561, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 97/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6579, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 98/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6509, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 99/110, Train loss: 0.0007, Train acc: 1.0000, Val loss: 2.6555, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 100/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6759, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 101/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6808, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 102/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6575, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 103/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6415, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 104/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6432, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 105/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6524, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 106/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6625, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 107/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6665, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 108/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6756, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 109/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6840, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Epoch: 110/110, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.6782, Val acc: 0.7901, Best Val loss: 2.4602, Best Val acc: 0.7901\n",
            "Test Accuracy for batch size 32, epochs 110: 80.6452%\n",
            "Training with batch size: 64, epochs: 90\n",
            "Epoch: 1/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0709, Val acc: 0.7901, Best Val loss: 2.0709, Best Val acc: 0.7901\n",
            "Epoch: 2/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0727, Val acc: 0.7901, Best Val loss: 2.0709, Best Val acc: 0.7901\n",
            "Epoch: 3/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0772, Val acc: 0.7901, Best Val loss: 2.0709, Best Val acc: 0.7901\n",
            "Epoch: 4/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0832, Val acc: 0.7901, Best Val loss: 2.0709, Best Val acc: 0.7901\n",
            "Epoch: 5/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0849, Val acc: 0.7901, Best Val loss: 2.0709, Best Val acc: 0.7901\n",
            "Epoch: 6/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0797, Val acc: 0.7901, Best Val loss: 2.0709, Best Val acc: 0.7901\n",
            "Epoch: 7/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0741, Val acc: 0.7901, Best Val loss: 2.0709, Best Val acc: 0.7901\n",
            "Epoch: 8/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0701, Val acc: 0.7901, Best Val loss: 2.0701, Best Val acc: 0.7901\n",
            "Epoch: 9/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0686, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 10/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0710, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 11/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0745, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 12/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0780, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 13/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0805, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 14/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0840, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 15/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0873, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 16/90, Train loss: 0.0006, Train acc: 1.0000, Val loss: 2.0872, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 17/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0833, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 18/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0819, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 19/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0828, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 20/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0832, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 21/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0847, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 22/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0862, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 23/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0893, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 24/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0910, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 25/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0932, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 26/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0960, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 27/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0983, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 28/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0981, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 29/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0953, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 30/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0941, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 31/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0960, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 32/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0997, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 33/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0981, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 34/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0956, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 35/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0964, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 36/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0996, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 37/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1021, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 38/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1026, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 39/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1018, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 40/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1006, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 41/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1025, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 42/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1050, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 43/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0986, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 44/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0958, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 45/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0957, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 46/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0984, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 47/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1015, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 48/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.0998, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 49/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1020, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 50/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1056, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 51/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1094, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 52/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1138, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 53/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1154, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 54/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1159, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 55/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1146, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 56/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1134, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 57/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1135, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 58/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1131, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 59/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1131, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 60/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1136, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 61/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1152, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 62/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1164, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 63/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1146, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 64/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1102, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 65/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1055, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 66/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1027, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 67/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1041, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 68/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1095, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 69/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1159, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 70/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1211, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 71/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1221, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 72/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1191, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 73/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1189, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 74/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1227, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 75/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1264, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 76/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1279, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 77/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1217, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 78/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1164, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 79/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1150, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 80/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1166, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 81/90, Train loss: 0.0005, Train acc: 1.0000, Val loss: 2.1189, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 82/90, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1201, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 83/90, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1229, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 84/90, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1263, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 85/90, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1282, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 86/90, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1279, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 87/90, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1282, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 88/90, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1302, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 89/90, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1328, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Epoch: 90/90, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1339, Val acc: 0.7901, Best Val loss: 2.0686, Best Val acc: 0.7901\n",
            "Test Accuracy for batch size 64, epochs 90: 77.4194%\n",
            "Training with batch size: 64, epochs: 100\n",
            "Epoch: 1/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1351, Val acc: 0.7901, Best Val loss: 2.1351, Best Val acc: 0.7901\n",
            "Epoch: 2/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1356, Val acc: 0.7901, Best Val loss: 2.1351, Best Val acc: 0.7901\n",
            "Epoch: 3/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1357, Val acc: 0.7901, Best Val loss: 2.1351, Best Val acc: 0.7901\n",
            "Epoch: 4/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1355, Val acc: 0.7901, Best Val loss: 2.1351, Best Val acc: 0.7901\n",
            "Epoch: 5/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1338, Val acc: 0.7901, Best Val loss: 2.1338, Best Val acc: 0.7901\n",
            "Epoch: 6/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1349, Val acc: 0.7901, Best Val loss: 2.1338, Best Val acc: 0.7901\n",
            "Epoch: 7/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1355, Val acc: 0.7901, Best Val loss: 2.1338, Best Val acc: 0.7901\n",
            "Epoch: 8/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1361, Val acc: 0.7901, Best Val loss: 2.1338, Best Val acc: 0.7901\n",
            "Epoch: 9/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1367, Val acc: 0.7901, Best Val loss: 2.1338, Best Val acc: 0.7901\n",
            "Epoch: 10/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1369, Val acc: 0.7901, Best Val loss: 2.1338, Best Val acc: 0.7901\n",
            "Epoch: 11/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1384, Val acc: 0.7901, Best Val loss: 2.1338, Best Val acc: 0.7901\n",
            "Epoch: 12/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1381, Val acc: 0.7901, Best Val loss: 2.1338, Best Val acc: 0.7901\n",
            "Epoch: 13/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1334, Val acc: 0.7901, Best Val loss: 2.1334, Best Val acc: 0.7901\n",
            "Epoch: 14/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1327, Val acc: 0.7901, Best Val loss: 2.1327, Best Val acc: 0.7901\n",
            "Epoch: 15/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1320, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 16/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1334, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 17/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1369, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 18/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1398, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 19/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1415, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 20/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1425, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 21/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1431, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 22/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1450, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 23/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1427, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 24/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1401, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 25/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1389, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 26/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1381, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 27/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1404, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 28/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1444, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 29/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1487, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 30/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1502, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 31/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1522, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 32/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1532, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 33/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1513, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 34/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1493, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 35/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1482, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 36/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1452, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 37/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1418, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 38/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1423, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 39/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1458, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 40/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1510, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 41/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1564, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 42/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1599, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 43/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1621, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 44/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1595, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 45/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1563, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 46/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1541, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 47/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1545, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 48/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1577, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 49/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1575, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 50/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1582, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 51/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1605, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 52/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1634, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 53/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1594, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 54/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1578, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 55/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1573, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 56/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1593, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 57/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1606, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 58/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1620, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 59/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1621, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 60/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1648, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 61/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1699, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 62/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1747, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 63/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1682, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 64/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1631, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 65/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1610, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 66/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1624, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 67/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1664, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 68/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1694, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 69/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1691, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 70/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1697, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 71/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1727, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 72/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1749, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 73/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1756, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 74/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1765, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 75/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1774, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 76/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1764, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 77/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1741, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 78/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1740, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 79/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1764, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 80/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1798, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 81/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1806, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 82/100, Train loss: 0.0004, Train acc: 1.0000, Val loss: 2.1778, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 83/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1754, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 84/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1763, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 85/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1799, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 86/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1809, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 87/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1829, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 88/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1854, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 89/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1821, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 90/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1785, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 91/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1783, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 92/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1798, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 93/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1813, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 94/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1799, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 95/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1815, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 96/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1857, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 97/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1900, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 98/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1945, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 99/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1917, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Epoch: 100/100, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1786, Val acc: 0.7901, Best Val loss: 2.1320, Best Val acc: 0.7901\n",
            "Test Accuracy for batch size 64, epochs 100: 77.4194%\n",
            "Training with batch size: 64, epochs: 110\n",
            "Epoch: 1/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1723, Val acc: 0.7901, Best Val loss: 2.1723, Best Val acc: 0.7901\n",
            "Epoch: 2/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1707, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 3/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1721, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 4/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1736, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 5/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1746, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 6/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1785, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 7/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1841, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 8/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1912, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 9/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1949, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 10/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1962, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 11/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1973, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 12/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1993, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 13/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1995, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 14/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2019, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 15/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2047, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 16/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2054, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 17/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2057, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 18/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2053, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 19/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2012, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 20/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1943, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 21/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1931, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 22/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1941, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 23/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1973, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 24/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2006, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 25/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1984, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 26/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.1994, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 27/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2036, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 28/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2081, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 29/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2136, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 30/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2112, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 31/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2082, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 32/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2062, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 33/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2020, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 34/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2001, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 35/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2020, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 36/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2061, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 37/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2107, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 38/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2125, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 39/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2116, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 40/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2121, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 41/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2143, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 42/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2183, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 43/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2166, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 44/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2138, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 45/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2103, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 46/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2073, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 47/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2074, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 48/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2107, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 49/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2146, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 50/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2163, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 51/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2177, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 52/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2208, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 53/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2223, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 54/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2219, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 55/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2175, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 56/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2159, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 57/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2141, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 58/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2133, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 59/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2162, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 60/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2202, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 61/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2237, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 62/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2266, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 63/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2300, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 64/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2301, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 65/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2295, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 66/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2294, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 67/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2307, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 68/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2298, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 69/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2253, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 70/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2200, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 71/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2193, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 72/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2207, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 73/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2231, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 74/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2256, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 75/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2275, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 76/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2303, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 77/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2307, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 78/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2319, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 79/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2355, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 80/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2389, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 81/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2376, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 82/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2317, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 83/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2285, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 84/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2287, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 85/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2315, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 86/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2339, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 87/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2349, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 88/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2381, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 89/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2387, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 90/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2398, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 91/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2425, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 92/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2427, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 93/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2419, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 94/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2426, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 95/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2445, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 96/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2459, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 97/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2468, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 98/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2462, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 99/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2466, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 100/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2454, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 101/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2468, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 102/110, Train loss: 0.0002, Train acc: 1.0000, Val loss: 2.2494, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 103/110, Train loss: 0.0002, Train acc: 1.0000, Val loss: 2.2472, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 104/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2410, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 105/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2392, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 106/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2396, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 107/110, Train loss: 0.0003, Train acc: 1.0000, Val loss: 2.2432, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 108/110, Train loss: 0.0002, Train acc: 1.0000, Val loss: 2.2467, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 109/110, Train loss: 0.0002, Train acc: 1.0000, Val loss: 2.2487, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Epoch: 110/110, Train loss: 0.0002, Train acc: 1.0000, Val loss: 2.2520, Val acc: 0.7901, Best Val loss: 2.1707, Best Val acc: 0.7901\n",
            "Test Accuracy for batch size 64, epochs 110: 77.4194%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4bb5389311ce4fd18ebbcfec2face2b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15a45c6a74df4e759878aedba3f25825",
              "IPY_MODEL_bc3424be552a4d3fb442d365d6723d0e",
              "IPY_MODEL_3ebf3ecdab6949e0a35442a7ac5348ff"
            ],
            "layout": "IPY_MODEL_688ff71d25e14d7481ee90ffa138ea99"
          }
        },
        "15a45c6a74df4e759878aedba3f25825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fed1353f5f247ba9f363eaaef6ba785",
            "placeholder": "​",
            "style": "IPY_MODEL_855973f826574a90b312fd17631cbfd9",
            "value": "100%"
          }
        },
        "bc3424be552a4d3fb442d365d6723d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a7d10743e054b1e82db5d2d5a07581a",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96a9f4febc1143f692d261370d5f80f5",
            "value": 100
          }
        },
        "3ebf3ecdab6949e0a35442a7ac5348ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d8f94edac58470fb0fcdfa4c41891dc",
            "placeholder": "​",
            "style": "IPY_MODEL_7d2eec2434f44fbfb77b511469b19fa3",
            "value": " 100/100 [00:02&lt;00:00, 58.61it/s]"
          }
        },
        "688ff71d25e14d7481ee90ffa138ea99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fed1353f5f247ba9f363eaaef6ba785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "855973f826574a90b312fd17631cbfd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a7d10743e054b1e82db5d2d5a07581a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96a9f4febc1143f692d261370d5f80f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d8f94edac58470fb0fcdfa4c41891dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d2eec2434f44fbfb77b511469b19fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}